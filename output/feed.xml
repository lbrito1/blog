<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="https://lbrito1.github.io/blog/">
  <id>https://lbrito1.github.io/blog/</id>
  <title>A Developer's Notebook</title>
  <updated>2021-12-30T12:34:36Z</updated>
  <link rel="alternate" href="https://lbrito1.github.io/blog/" type="text/html"/>
  <link rel="self" href="https://lbrito1.github.io/blog/feed.xml" type="application/atom+xml"/>
  <author>
    <name>Leonardo Brito</name>
    <uri>https://lbrito1.github.io</uri>
  </author>
  <entry>
    <id>tag:lbrito1.github.io,2021-12-30:/blog/2021/12/botched_interviews.html</id>
    <title type="html">Botched interviews</title>
    <published>2021-12-30T12:34:36Z</published>
    <updated>2021-12-30T12:34:36Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2021/12/botched_interviews.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2021/12/botched_interviews.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2021/puerto-varas-sm.jpg" alt="Sunset in Puerto Varas."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/puerto-varas-sm.jpg" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Here’s something I’ve been wanting to write for a while: all the times (the ones I can remember, anyway) I bombed a software engineer job interview. There are so many “how I aced interviewing at X”/”how to pass X interview” floating around that I thought the opposite story would make for an amusing read.&lt;/p&gt;

&lt;p&gt;My first developer job was as an intern at a big tech company in 2012. I think that was one of the worst interviews I’ve had, by the way – I could barely understand the interviewer over the cellphone, and those were the days of “how many piano players are there in New York”-kind of questions. I thought it went terrible, but I got the job somehow. On the other hand I’ve had many interviews I thought I did great but bombed anyway.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id="faang-1-2013"&gt;FAANG 1 (2013)&lt;/h2&gt;

&lt;p&gt;I was just out of school when I got a cold call from a FAANG recruiter asking if I was interested in interviewing there. Needless to say I was naive and didn’t quite know what I was getting myself into.&lt;/p&gt;

&lt;p&gt;The first part of the process was a phone screen with a technical recruiter. She asked me something like “What is the fastest way to sum two 32-bit integers?”. I froze like a deer in the headlights.&lt;/p&gt;

&lt;p&gt;After gasping in awe at the bizarre question and mumbling some nonsense, I was informed that my answer was &lt;em&gt;not&lt;/em&gt; correct. The &lt;em&gt;right&lt;/em&gt; answer, the recruiter said, was “using the CPU’s TLB”. The &lt;a href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer"&gt;translation lookaside buffer&lt;/a&gt; is a memory cache located between the CPU and the CPU cache. I still had college classes in my somewhat-recent memory at the time, so I kind of knew that this thing existed, but to this day I still don’t know how to sum two integers with it.&lt;/p&gt;

&lt;h2 id="faang-1-2014"&gt;FAANG 1 (2014)&lt;/h2&gt;

&lt;p&gt;A year passed and (the same) FAANG came to my town with a recruitment event. Again I got a call, and instead of a phone screening, I would go straight to the event location and do a quick onsite interview.&lt;/p&gt;

&lt;p&gt;They provided recommendations on technical subjects I should refresh my memory about: basically Algorithms 101 syllabus; sorting algorithms, red/black and AVL trees, A* and Dijkstra, NP-complete problems, etc, as well as some operating systems topics: processes, threads, mutexes, scheduling algorithms and so on.&lt;/p&gt;

&lt;p&gt;At the time I had just started grad school. I was a bit more seasoned than the last interview, but far from having any relevant industry experience.&lt;/p&gt;

&lt;p&gt;Interview day. I got to the venue and sat reading my notes on how to balance AVL trees. The interviewers showed up, greeted me and got things started. They gave me pen and paper and asked me how to some things on a list of integers. I don’t recall the interview being particularly bad or anything, but round one was the end of the line for me. A few days later I got a boilerplate rejection email and that was my last contact with this FAANG.&lt;/p&gt;

&lt;h2 id="mid-sized-tech-2014"&gt;Mid-sized tech (2014)&lt;/h2&gt;

&lt;p&gt;Grad school classes were few and far apart, so I decided to start looking for a job. This mid-sized tech company had a local office, so I got in touch and scheduled an interview.&lt;/p&gt;

&lt;p&gt;I don’t remember any meaningful details from this interview. One thing I do remember is being asked what monthly compensation I expected. The interviewer passed me a scrap of paper and a pen for me to write the number down. I took a few moments to think and wrote down what I thought was an adequate number. In today’s US dollars, that number would be enough to afford a parking spot in San Francisco, but it was an okay salary for a junior hire in my town.&lt;/p&gt;

&lt;p&gt;I didn’t get a lot of feedback here other than “we went with someone else”.&lt;/p&gt;

&lt;h2 id="faang-2-2019"&gt;FAANG 2 (2019)&lt;/h2&gt;

&lt;p&gt;A few years had passed since my last failure. I had finished my education, become a Ruby developer and enjoyed a 3.5-years tenure at a great, small local software studio. I had a preferred text editor. I had dotfiles. I felt weathered. So I did what you’re supposed to do: I applied to FAANG.&lt;/p&gt;

&lt;p&gt;FAANG responded to my application, and after a couple of &lt;em&gt;months&lt;/em&gt; and being ghosted by one of the 5+ recruiters involved in the process, things got on track for the onsite.&lt;/p&gt;

&lt;p&gt;I did the Leetcode thing daily. I read Glassdoor tips and talked to college friends that worked at this company. I even went on Blind and found out that you’re basically an idiot if you don’t pass FAANG’s interview, cause it’s so damn easy.&lt;/p&gt;

&lt;p&gt;I passed the initial screening rituals and was invited for an onsite – 25 hours and 3 flights away. As I obsessively went through my notes in the hotel, I thought I was finally &lt;em&gt;ready&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There were maybe 5 or 6 rounds, each with one or two interviewers, all very nice and moderately helpful. Pretty much textbook FAANG interview, just like CTCI describes it. I was asked a particularly difficult set of questions. Mid-interview, I got the familiar feeling that a train is steaming towards me and I’ll soon be smashed into smithereens. Looking back, I’d evaluate myself at this interview as Not Good.&lt;/p&gt;

&lt;p&gt;Another 25 hours, 2-stop travel back to my town. Feedback came swiftly in the familiar lukewarm rejection phone call.&lt;/p&gt;

&lt;h2 id="big-tech-2019"&gt;Big Tech (2019)&lt;/h2&gt;

&lt;p&gt;This Big Tech company is highly respected in the Ruby community and their culture seemed to align with my own. I tried, and failed, their interview process two times.&lt;/p&gt;

&lt;p&gt;They have a fairly straightforward interview process: first a phone screen/short code challenge, then a longer behavioral/technical challenge, typically onsite. First time, I didn’t make it to the onsite.&lt;/p&gt;

&lt;p&gt;My second attempt was much smoother and lead to the onsite. Like &lt;em&gt;FAANG 1 (2019)&lt;/em&gt;, this involved multiple flights and 20+ hours of travel. Also like my previous interview, I &lt;em&gt;felt ready&lt;/em&gt;. The recruiter was great, and every person I met so far was extremely nice. Company culture seemed fantastic and I liked the tech stack.&lt;/p&gt;

&lt;p&gt;There were two technical rounds, one cultural fit conversation and one technical-but-not-coding round. The coding parts went well, maybe a B+. The culture fit thing was very good. The not-coding part, ironically the one I had prepared the most, was a disaster, although I didn’t see things that way at the time.&lt;/p&gt;

&lt;p&gt;I came prepared to talk about one of the projects I lead in my current job at the time; evidently I had an NDA in place and had to navigate around it. I thought this was fine – I had already published a post on the same subject on the company’s public blog and never had any complaints about it being too esoteric/abstract. The interviewer was not amused by this at all. Because of the vagueness of the language I was using, he seemed to think I was describing something shady. At one point, he actually asked something like “do your users know you are doing this”! Right now I think that was kind of funny, but I was utterly bewildered at the time. I tried to reassure him that there was nothing fishy going on, but at that point he had probably made his mind. I might as well have gone straight to the airport and saved everyone some time.&lt;/p&gt;

&lt;p&gt;Rejection call came a week later. Upon my request, the recruiter followed with a very thorough email detailing the reasons of the rejection, which is a fairly unusual thing for these companies to do, and very helpful for the candidate. Although I think the interviewer could have handled the situation better (just ask me to describe another project), I have great respect for the way the company handled the process and gave honest feedback.&lt;/p&gt;

&lt;h2 id="the-printer-in-the-room"&gt;The printer in the room&lt;/h2&gt;

&lt;p&gt;Hiring is the printer of software engineering jobs. It kind of works, but not very well, and everyone seems to agree it should be better at this point. This is in no way a demerit to recruiters – they’re doing their job and are not at fault here. They’re usually pretty good; its the framework that isn’t great. There have been some incremental improvements: code sharing platforms are pretty good, which makes remote interviewing very straightforward (and reduces the need for onsites, &lt;a href="https://lbrito1.github.io/blog/2021/08/onsites.html"&gt;which suck&lt;/a&gt;). There are many services where you build a single profile and apply to many companies at once, which reduces the time waste of filling the same forms in all the different companies’ websites. The bulk of the process remains more or less the same though.&lt;/p&gt;

&lt;p&gt;Interviewing is in part a numbers game, but also not &lt;em&gt;entirely&lt;/em&gt; random, which means there is a way to get better at it (that’s the whole point of companies like Leetcode and books like CTCI). Failing an interview you prepared for leaves a sour taste in the mouth, but over time it gets easier to accept as just part of the probability game.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2021-10-07:/blog/2021/09/job_offers_pandemic.html</id>
    <title type="html">Analyzing LinkedIn's data export: what happened in 2021?</title>
    <published>2021-10-07T14:45:00Z</published>
    <updated>2021-10-07T14:45:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2021/09/job_offers_pandemic.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2021/09/job_offers_pandemic.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2021/linkedin-wordcloud.png" alt="Bar chart showing distribution of jobs I applied to per country. US ranks first with over 10 applications."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/linkedin-wordcloud.png" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;I’ve been using LinkedIn basically since I started working as an intern back in 2012. My usage is mostly limited to posting my blog posts, except the couple of times I used the platform to search for a new job. So most of the time, LinkedIn has been pretty slow-paced, with maybe half a dozen random recruiters reaching out per year.&lt;/p&gt;

&lt;p&gt;However, since the Covid-19 pandemic started, and particularly in 2021, things seem to have gone a little crazy, with a &lt;em&gt;lot&lt;/em&gt; more recruiter activity. I was curious to see just how much things had changed, so I looked at LinkedIn’s data export.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;First I requested my data from LinkedIn:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2021/linkedin-request.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2021/linkedin-request.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/linkedin-request.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Messages, Connections and Invitations seem like the most promising sources of data:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;➜  Basic_LinkedInDataExport_10-01-2021 &lt;span class="nb"&gt;ls&lt;/span&gt; &lt;span class="nt"&gt;-lahtS&lt;/span&gt;
total 1020K
&lt;span class="nt"&gt;-rw-rw-r--&lt;/span&gt; 1 leo leo 577K out  1 13:06  messages.csv
&lt;span class="nt"&gt;-rw-rw-r--&lt;/span&gt; 1 leo leo 146K out  1 13:05  Connections.csv
&lt;span class="nt"&gt;-rw-rw-r--&lt;/span&gt; 1 leo leo 113K out  1 13:05  Contacts.csv
&lt;span class="nt"&gt;-rw-rw-r--&lt;/span&gt; 1 leo leo  43K out  1 13:05  Learning.csv
&lt;span class="nt"&gt;-rw-rw-r--&lt;/span&gt; 1 leo leo  23K out  1 13:05  Invitations.csv&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The Connections export is somewhat limited for our purposes: I only actively add people on LinkedIn during a job search.&lt;/p&gt;

&lt;p&gt;Messages are a bit more interesting because a lot of recruiters immediately offer a position in their first contact (sometimes even with a pre-scheduled Google calendar event! I wish things were this straightforward back when I was finishing school).&lt;/p&gt;

&lt;p&gt;Invites are also a good source, complimentary to Messages. After accepting or rejecting an invite, the Invitation is deleted, so there’s no danger of double counting an interaction that started as Invitation and then evolved to Messaging.&lt;/p&gt;

&lt;p&gt;Focusing first on the Messages export, here are some relevant info we might aspire to extract:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Job offers (as in “I have a job I want you to apply for”) per date&lt;/li&gt;
  &lt;li&gt;Keywords mentioned in messages (“Ruby”, “Rails” etc)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s see if we can extract those.&lt;/p&gt;

&lt;h2 id="job-offers-per-month"&gt;Job offers per month&lt;/h2&gt;

&lt;p&gt;Job offers mainly come from messages, and the bulk of my messages come from recruiters. However, I do get a few scattered personal messaging from old acquaintances, some professional but not interview-related conversations, etc.&lt;/p&gt;

&lt;p&gt;A simple approach to estimate how many messages are actually from someone promoting a job opening is to look for certain job-related terms: in my case, as a Ruby engineer, if a message contains “Ruby” it is probably from a recruiter advertising a Ruby-related position. This is only an estimate: maybe I chatted about Ruby at some point with an acquaintance, which of course is non-related to our objective here. Those cases are few and far apart compared to the recruiter conversations though.&lt;/p&gt;

&lt;p&gt;With that in mind, I built a list of terms that are related to job searches:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-ruby"&gt;&lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="n"&gt;offer&lt;/span&gt; &lt;span class="n"&gt;opportunity&lt;/span&gt; &lt;span class="n"&gt;ruby&lt;/span&gt; &lt;span class="n"&gt;developer&lt;/span&gt; &lt;span class="n"&gt;engineer&lt;/span&gt; &lt;span class="n"&gt;talent&lt;/span&gt; &lt;span class="n"&gt;salary&lt;/span&gt; &lt;span class="n"&gt;relocation&lt;/span&gt; &lt;span class="n"&gt;position&lt;/span&gt; &lt;span class="n"&gt;role&lt;/span&gt; &lt;span class="n"&gt;recruiter&lt;/span&gt; &lt;span class="n"&gt;talent&lt;/span&gt; &lt;span class="n"&gt;looking&lt;/span&gt; &lt;span class="n"&gt;interested&lt;/span&gt; &lt;span class="n"&gt;oportunidade&lt;/span&gt; &lt;span class="n"&gt;trabalho&lt;/span&gt; &lt;span class="n"&gt;vaga&lt;/span&gt; &lt;span class="n"&gt;software&lt;/span&gt; &lt;span class="n"&gt;experience&lt;/span&gt; &lt;span class="n"&gt;tech&lt;/span&gt; &lt;span class="n"&gt;rails&lt;/span&gt; &lt;span class="n"&gt;interesse&lt;/span&gt; &lt;span class="n"&gt;interested&lt;/span&gt; &lt;span class="n"&gt;company&lt;/span&gt; &lt;span class="n"&gt;email&lt;/span&gt; &lt;span class="n"&gt;work&lt;/span&gt; &lt;span class="n"&gt;senior&lt;/span&gt; &lt;span class="n"&gt;contato&lt;/span&gt; &lt;span class="n"&gt;vagas&lt;/span&gt; &lt;span class="n"&gt;remote&lt;/span&gt; &lt;span class="n"&gt;working&lt;/span&gt; &lt;span class="n"&gt;stack&lt;/span&gt; &lt;span class="n"&gt;backend&lt;/span&gt; &lt;span class="n"&gt;technical&lt;/span&gt; &lt;span class="n"&gt;developers&lt;/span&gt; &lt;span class="n"&gt;skill&lt;/span&gt; &lt;span class="n"&gt;skills&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Most of the messaging I get is in English, but I do get a significant amount of contacts in Portuguese as well, so we have terms in both languages.&lt;/p&gt;

&lt;p&gt;With that list of terms, we can simply &lt;code&gt;select&lt;/code&gt; all the relevant ones and group those by month:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-ruby"&gt;  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;job_messages_per_month&lt;/span&gt;
    &lt;span class="n"&gt;job_related_messages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="vi"&gt;@input&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;select&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"CONTENT"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=~&lt;/span&gt; &lt;span class="sr"&gt;/&lt;/span&gt;&lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="vi"&gt;@relevant_words&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="sr"&gt;/i&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;metric_per_month&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;job_related_messages&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="no"&gt;Time&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"DATE"&lt;/span&gt;&lt;span class="p"&gt;]).&lt;/span&gt;&lt;span class="nf"&gt;to_date&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, when I’m not actively looking for another job, I tend not do look at LinkedIn too much, so the Invitations tend to pile up. As already mentioned, accepted/rejected invites get “deleted” from LinkedIn’s data export (which doesn’t seem like a great practice IMO, as they probably still have that data), so only invites that you haven’t acted on either way are available in the CSV export.&lt;/p&gt;

&lt;p&gt;Just like with messages, we group the relevant invites (“Inbound”, meaning someone is adding you as opposed to “Outbound” where you’re adding someone) by date. I didn’t bother filtering by terms because nearly everyone that adds me is a recruiter these days:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-ruby"&gt;  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;recruiters_per_month&lt;/span&gt;
    &lt;span class="n"&gt;received_invites&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="vi"&gt;@input&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;select&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Direction"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"INCOMING"&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;metric_per_month&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;received_invites&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="no"&gt;Time&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;strptime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Sent At"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s2"&gt;"%m/%d/%y"&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nf"&gt;to_date&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here’s the result of summing both data, messages and invites:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2021/linkedin-offers-month.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2021/linkedin-offers-month.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/linkedin-offers-month.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;There was a spike in early 2019, when I actively pursued a new job. I also gave a &lt;a href="http://www.thedevelopersconference.com.br/tdc/2019/florianopolis/trilha-web-frontend"&gt;conference talk&lt;/a&gt; at that time and added a bunch of people on LinkedIn. Thus, this peak in job offers is just a consequence of me actively looking for a job. After that, activity dropped to back to lower levels (I also ticked “not looking for a job” on LinkedIn right after I signed the offer at my new job around April 2019).&lt;/p&gt;

&lt;p&gt;On the other hand, since late 2020 job offer messaging has grown steadily. I wasn’t actively looking, so this here is organic growth. I’m curious to see if other people also had a similar pattern. Perhaps this is a reflection of an increase of demand in some specific skillset (Ruby) or experience level (X years of experience), but I’m guessing its part of a general upwards trend in the industry since the beginning of the pandemic.&lt;/p&gt;

&lt;h2 id="most-common-terms"&gt;Most common terms&lt;/h2&gt;

&lt;p&gt;Another interesting piece of information is the most common words mentioned in the conversations.&lt;/p&gt;

&lt;p&gt;We could just count how frequently each word pops up, but irrelevant words like “the”, “a” and so on would rank in the top. So first we need to get rid of those words, then look at the linted text. I’m sure there’s an API that does just that somewhere out there, but I created my own list of non-relevant words manually and used that instead.&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-ruby"&gt;  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;word_frequencies&lt;/span&gt;
    &lt;span class="n"&gt;full_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="vi"&gt;@input&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;map&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"CONTENT"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}.&lt;/span&gt;&lt;span class="nf"&gt;compact&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;" "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;normalize_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_text&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sr"&gt;/\s+/&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
      &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;map&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;gsub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sr"&gt;/[^a-z]+/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;reject&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;size&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="vi"&gt;@nonrelevant_words&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;include?&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;group_count&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;sort_by&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}.&lt;/span&gt;&lt;span class="nf"&gt;reverse&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Using the &lt;a href="https://github.com/zverok/magic_cloud"&gt;MagicCloud&lt;/a&gt; gem, here’s the plotted results:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2021/linkedin-wordcloud.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2021/linkedin-wordcloud.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/linkedin-wordcloud.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;No surprises there – terms like “Ruby” and “Rails” are among the most frequent. Other bland job-related terms compose the bulk of the word cloud.&lt;/p&gt;

&lt;p&gt;Here are the actual numbers for these terms:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2021/linkedin-terms.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2021/linkedin-terms.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/linkedin-terms.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;h2 id="what-happenedis-happening"&gt;What happened/is happening?&lt;/h2&gt;

&lt;p&gt;Going back to the original question: what happened in 2021? Did software-related job offers explode during the pandemic? My anecdata points to an obvious Yes. Most articles discussing this question are just opinion pieces around the lines of “engineering demand increased because digital services sharply expanded with the lockdowns”. I couldn’t find any hard data supporting these assumptions (other than this personal analysis presented above).&lt;/p&gt;

&lt;p&gt;One of the major impacts of the pandemic, being forced to remote-only did have pretty obvious effects in non-US job markets. Before the pandemic, I suspect that many very competent people were hesitant to leave their jobs due to strictly non-remote perks: nice offices, work colleagues, local benefits like healthcare, maybe even specific labor laws regarding vacation time and so on.&lt;/p&gt;

&lt;p&gt;Remote jobs based in strong currency countries, especially the US, were already “a thing” long before the pandemic, but with remote work being mandatory rather than an option, local remote vs foreign remote boils down to a huge pay gap in most cases, with US-based software engineering salaries being hard to compete with anywhere else in the world.&lt;/p&gt;

&lt;p&gt;I’m very curious to see how this pans out for local software shops. These local companies are really bleeding talent leaving for stronger currencies; if these dynamics go on for too long, I can’t see how most of them will last.&lt;/p&gt;

&lt;h2 id="code"&gt;Code&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://github.com/lbrito1/LinkedIn-insights"&gt;Here’s the repo&lt;/a&gt; with all the scripts needed to reproduce these results.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2021-08-27:/blog/2021/08/onsites.html</id>
    <title type="html">Onsites considered harmful</title>
    <published>2021-08-27T21:33:00Z</published>
    <updated>2021-08-27T21:33:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2021/08/onsites.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2021/08/onsites.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2021/onsite-sm.jpg" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/onsite-sm.jpg" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;A couple of years ago I interviewed at one of the largest Ruby shops out there. Screening went well, and some days later I was invited for an onsite.&lt;/p&gt;

&lt;p&gt;These were the good old pre-covid days, so an onsite really meant &lt;em&gt;onsite&lt;/em&gt;. You had to travel to the office, wherever that was.&lt;/p&gt;

&lt;p&gt;The thing is, an onsite is actually radically different depending on &lt;em&gt;where you live&lt;/em&gt;. It follows that onsites introduce further bias into our industry’s already problematic hiring process. I’d like to argue that although onsites have some advantages, they’re mostly a waste of time (and money).&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;If you’re a local, an onsite probably means taking a bus, metro, taxi, walking, whatever. If you’re not local but are at least within the same country that you’re interviewing, it might take a day trip or maybe a short flight. If you’re a foreigner it might take travel visas and a week of travel.&lt;/p&gt;

&lt;p&gt;Without getting too philosophical, we all know we have a limited amount of sand grains in our hourglass. &lt;a href="https://www.wikiwand.com/en/Sunk_cost"&gt;Fallacies apart&lt;/a&gt;, anyone can &lt;em&gt;feel&lt;/em&gt; that the more we pour into something – be it renovating a kitchen or traveling for an onsite – the higher the stakes become.&lt;/p&gt;

&lt;p&gt;Its glaringly obvious that someone who invested a 30 minute bus ride to an onsite will be much more at ease than someone who flew godless hours on a red-eye. It doesn’t really matter how much pampering the latter is treated with: exquisite hotels, meal allowances… investing a week of your time will always drive up anxiety a lot more than taking an afternoon off work.&lt;/p&gt;

&lt;p&gt;Back to my story: I was interviewing for a company overseas. I happened to have a valid visa for that country, which already puts me in some advantage compared to others. Physically getting to the company building for the interview, however, took some effort: I drove to the local airport, where I arrived &lt;a href="https://www.theonion.com/dad-suggests-arriving-at-airport-14-hours-early-1819573933"&gt;more than a couple of hours early&lt;/a&gt;, flew down to São Paulo, then took two more flights until I reached my final destination, some &lt;em&gt;thirty hours&lt;/em&gt; after I stepped out of my house, then I took another cab to the fancy hotel booked by my not-to-be future company and collapsed for the night.&lt;/p&gt;

&lt;p&gt;Next day I had the onsite (which took basically the full business hours), then back to the hotel, collapsed again. On the third day I backtracked through the 30 hours of cabs, airports and flights back home. This was late December, by the way, so airports were &lt;em&gt;packed&lt;/em&gt;. A couple of days later, on Christmas eve, I got a very thoughtful &lt;em&gt;happy holidays + no thanks&lt;/em&gt; call from the recruiter.&lt;/p&gt;

&lt;p&gt;Might I have gotten the job if I had taken a bus instead of multiple planes? Maybe, maybe not (probably not, since someone in the interview panel just didn’t enjoy my parlance).&lt;/p&gt;

&lt;p&gt;That isn’t really the point, though, and as far as &lt;em&gt;anecdata&lt;/em&gt; goes, I have the opposite story as well: I interviewed twice at the same company, once through a tortuous voyage similar to the one I described above, and another time at my city, with the roles reversed: I left my house and drove for a few minutes to the onsite, while the interviewers were enjoying a fancy beachside hotel after several plane trips. I failed the former and got an offer from the latter. I’m the same person applying for the same job at the same company. Did I just perform better? Did they perform “worse”? Is it all just a coincidence? Are all of these interviews meaningless hazing rituals?&lt;/p&gt;

&lt;p&gt;But I digress. Back to the matter at hand: if you think of it, the &lt;em&gt;60 hours&lt;/em&gt; of commuting alone is more than one work week (and as far as effort goes, I’m sure more goes into enduring 60 hours of planes and airports than into programming). If you factor in the actual onsite, then we’re talking about &lt;em&gt;two workweeks&lt;/em&gt; of effort put into a no-strings-attached situation. The elapsed wall time is well into a full business week.&lt;/p&gt;

&lt;p&gt;There are sensible, rational grounds for an onsite. Recruiters want to know if the candidate hates the cold and is going to churn early winter, or maybe the city is too small, or too big, or whatever random factor might make people want to run away. That said, I find it hard to believe even the most prescient can get a read on any of those thoughts rushing through the candidate (probably even the candidate can’t).&lt;/p&gt;

&lt;p&gt;In any case, are those things worth the several thousand dollars of expenses, and more importantly, are they worth excluding a possibly large pool of candidates that aren’t willing to invest a full week of their time on a process with &lt;a href="https://blog.interviewing.io/technical-interview-performance-is-kind-of-arbitrary-heres-the-data/"&gt;naturally low chances of going forward&lt;/a&gt;? At least in my opinion, those very thin pros are outshined by the very real cons like the &lt;code&gt;-8.208527&lt;/code&gt; Sun outshines my laptop screen.&lt;/p&gt;

&lt;p&gt;Now let us also remember that the success of job searches depends on arbitrary things like if everyone on the panel likes your face. We all know things shouldn’t be this way; we’re supposed to be unbiased and empathetic, but let’s face it – we humans &lt;a href="https://www.sciencedaily.com/releases/2020/07/200714101228.htm"&gt;&lt;em&gt;suck&lt;/em&gt;&lt;/a&gt; at that.&lt;/p&gt;

&lt;p&gt;Even if we consider an utopically unbiased interviewer panel, there’s still all sorts of random noise going on at an interview, like performance anxiety. No matter how great the people interviewing are, and even how great &lt;em&gt;you&lt;/em&gt; are, interviewing always has a huge degree of uncertainty:&lt;/p&gt;

&lt;blockquote&gt;"The fact that people who are overall pretty strong (e.g. mean ~= 3) can mess up technical interviews as much as 22% of the time shows that there’s definitely room for improvement in the process"&lt;a href="https://blog.interviewing.io/technical-interview-performance-is-kind-of-arbitrary-heres-the-data/"&gt;
&lt;br&gt;- Technical interview performance is kind of arbitrary. Here’s the data.&lt;/a&gt;
&lt;/blockquote&gt;

&lt;p&gt;My point is: such a volatile thing should &lt;strong&gt;never&lt;/strong&gt; have been tied to multiple plane tickets and 2-night hotel stays in a different continent in the first place.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Never&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Perhaps surprisingly, “onsites” are still a thing during the covid pandemic – they’re just remote, i.e., &lt;em&gt;not really onsites at all&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This is immensely beneficial for everyone involved: the company won’t have to pay for expensive hotels and plane tickets, the planet won’t have to suffer the huge CO2 emissions from this ultimately unnecessary shenanigan, and the candidate won’t have to waste a week of his/her vacation time with something as ethereal as pursuing a software engineering job.&lt;/p&gt;

&lt;p&gt;Recruitment in this industry is difficult. This is widely acknowledged by all parts involved. No wonder there are so many books, videos and Discord channels about interviewing for a tech job – not to mention coding prep services, automated third-party code challenges… the list goes on.&lt;/p&gt;

&lt;p&gt;This post is specifically about onsites, but it is impossible not to mention the overall sad state of interviewing for a software engineer job. A quick survey of HN posts is enough to glimpse how people feel about this:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2021/hiring-1.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2021/hiring-1.png" alt="Search results for 'hiring is broken' on Algolia's HN search page."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/hiring-1.png" alt="Search results for 'hiring is broken' on Algolia's HN search page."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Sounds like hiring isn't in a great shape.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Inefficient and biased as it is (or, hopefully, &lt;em&gt;was&lt;/em&gt;), physical onsites are nowhere near the worst possible interviewing practice we can observe in the wild.&lt;/p&gt;

&lt;p&gt;Why interview for a job in a quiet office full of nerds if you can &lt;strong&gt;FIGHT FOR IT IN A TOURNAMENT&lt;/strong&gt; like a geek gladiator?&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2021/hiring-tournament.jpg" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2021/hiring-tournament.jpg" alt="An email from a company called BairesDev inviting me to fight for a job in a coding tournament."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/hiring-tournament.jpg" alt="An email from a company called BairesDev inviting me to fight for a job in a coding tournament."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Yikes.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Things aren’t any better on the other side of the table – finding skilled developers in 2021 is tough, even if you’re not setting up a pair programming arena for a code to the death contest.&lt;/p&gt;

&lt;p&gt;Some recruiters go a step beyond cold calling and start &lt;em&gt;cold referral calling&lt;/em&gt;, like this recruiter asking me to &lt;em&gt;pleeeeeeeeeease&lt;/em&gt; refer candidates that match their laundry list of requirements:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2021/hiring-pleeeeaase.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2021/hiring-pleeeeaase.png" alt="A recruiter message on Linkedin asking me to refer candidates, PLEEEEEEEASE."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/hiring-pleeeeaase.png" alt="A recruiter message on Linkedin asking me to refer candidates, PLEEEEEEEASE."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Pleeeeeeeeeease send me candidates that match my laundry list of skills!&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;My second point: recruitment is seriously hard for all parts involved. If we are able to, we should try to make it &lt;em&gt;easier&lt;/em&gt;, not &lt;em&gt;harder&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Onsites were a significant hassle on top of an already complicated, inefficient, time-consuming, stressful process.&lt;/p&gt;

&lt;p&gt;Although the company usually takes on most of all of the financial hit, the time and emotional load was carried by the candidate alone.&lt;/p&gt;

&lt;p&gt;Getting rid of physical onsites is &lt;em&gt;fantastic&lt;/em&gt; news for everyone – especially people interviewing, but also companies that can now cast a wider net and carry out a faster, more diverse recruitment process. And our planet will also have God-knows-how-many thousand tonnes of CO2 less to deal with each year.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2021-08-20:/blog/2021/08/efficient_resource_distribution.html</id>
    <title type="html">Efficient resource distribution</title>
    <published>2021-08-20T11:53:10Z</published>
    <updated>2021-08-20T11:53:10Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2021/08/efficient_resource_distribution.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2021/08/efficient_resource_distribution.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2021/resource-distribution.jpg" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/resource-distribution.jpg" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt; A simple metrics-based ranking system is good enough to decide who gets how many resources.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Computational resources – CPU time, memory usage, network traffic etc – are limited. This may be more or less of a problem depending on project/company size and so on; if you’re working on a smaller product with limited traffic, it might not be meaningful at all.&lt;/p&gt;

&lt;p&gt;Once past a certain threshold though, expenses with such resources become non-trivial and it begins to make sense to spend some time thinking about how to distribute them as efficiently as possible.&lt;/p&gt;

&lt;p&gt;Here’s the problem that got me thinking about this: at work, we had a computational resource that needed to be consumed by a large fleet of workers (think several thousand concurrent), but each &lt;em&gt;type&lt;/em&gt; of worker had different &lt;em&gt;productivity&lt;/em&gt;, and that productivity changed over time. How can we decide who gets what?&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;So the problem is: you have a set of &lt;em&gt;consumers&lt;/em&gt; that use the &lt;em&gt;same resource&lt;/em&gt;, for which you have a static budget. The consumers all solve the same problem, more or less (i.e. have the same &lt;em&gt;output&lt;/em&gt;), but come in different &lt;em&gt;types&lt;/em&gt; that have different &lt;em&gt;productivities&lt;/em&gt; (defined as &lt;em&gt;output per resource consumption&lt;/em&gt;). Additionally, although the consumer types solve the same problem, we want consumers to be as diverse as possible – we can’t just pick the best performing one and go with that.&lt;/p&gt;

&lt;p&gt;First thought that comes to mind is &lt;em&gt;this seems ordinary enough; there must be an easy, well-known solution&lt;/em&gt;. There might be, but I couldn’t find any that was simple and effective for this use case. Closest I got were &lt;a href="https://www.wikiwand.com/en/PID_controller"&gt;PID controllers&lt;/a&gt;, which solve a similar problem, but probably doesn’t solve the entire problem here (and also seems complicated).&lt;/p&gt;

&lt;p&gt;I gave the problem some thought and came up with a reasonable solution that has been working well for a year now.&lt;/p&gt;

&lt;p&gt;The problem boils down to two parts:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Consistently keeping track of productivity among the different consumer types;&lt;/li&gt;
  &lt;li&gt;Deciding how to share the resource among the consumers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The concept that glues both parts is that of the &lt;em&gt;cycle&lt;/em&gt; – a repeating time period in which we measure productivity and distribute resources to be shared within that time frame, until the next cycle comes up and everything is recalculated.&lt;/p&gt;

&lt;p&gt;Problem 1) boils down to maintaining a time series of how much output per resource each consumer type produced during the latest cycle.&lt;/p&gt;

&lt;p&gt;Problem 2) comes almost as a corollary to the former problem: we want the best global output possible, and that can be guessed by using the productivity stats from the previous cycle. This won’t be perfect, because productivity varies over time within each consumer type, but basic &lt;a href="https://www.wikiwand.com/en/Volatility_clustering"&gt;statistical intuition&lt;/a&gt; says it will be good enough for our purposes.&lt;/p&gt;

&lt;p&gt;So the first step of solving 2) is building a &lt;em&gt;ranking of consumers by productivity&lt;/em&gt;. We want a diverse set of consumer types, though, so we can’t just pick &lt;code&gt;type #1&lt;/code&gt; and give it 100% of the resources all the time. Also, the ranking might change each cycle, and we don’t want resource distribution to be too volatile – that might become hard to monitor and debug. We want something that is somewhat smooth, stable, convergent, but at the same time that reflects changes in productivity as quickly as possible, and that delivers good global output-per-resource-consumption.&lt;/p&gt;

&lt;p&gt;We know that the top tier within the ranking probably deserves more than the the rest, while the bottom tier probably deserves less, and that is the gist of the solution to problem 2). We don’t know beforehand how each consumer will perform though, so it makes sense to start with equal resource distribution among them.&lt;/p&gt;

&lt;p&gt;Here’s the complete solution I worked out:&lt;/p&gt;

&lt;p&gt;Start the system by sharing an equal amount of resources among all consumers: let’s say every consumer has the same weight &lt;em&gt;W&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Then, for each cycle:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Build the productivity-per-consumer-class ranking&lt;/li&gt;
  &lt;li&gt;For the top &lt;em&gt;N%&lt;/em&gt; consumers, do &lt;em&gt;W += K&lt;/em&gt; (limited to a certain maximum)&lt;/li&gt;
  &lt;li&gt;For the bottom &lt;em&gt;N%&lt;/em&gt; consumers, do &lt;em&gt;W -= K&lt;/em&gt; (limited to a certain minimum)&lt;/li&gt;
  &lt;li&gt;Translate each &lt;em&gt;W&lt;/em&gt; to a real-world resource amount (e.g. “1GB RAM” or something). This involves the weights as well as the global resource budget per time cycle, such that we guarantee we won’t exceed the static budget.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The global sum of weights is kept more or less the same because we’re summing and subtracting the same amounts each cycle (although this isn’t perfect because we have min and max values), so the system is kept fairly stable over time while also reacting quickly to changes in productivity. Also, the system is robust, and blowing up the weights store is no big deal – weights will creep back to their previous values over a short time.&lt;/p&gt;

&lt;p&gt;To finalize, here’s a chart showing the weights of different types of consumers over the last few months:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2021/grafana.jpg" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2021/grafana.jpg" alt="Dashboard showing a chart with several colored lines representing the weights of each consumer class."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2021/grafana.jpg" alt="Dashboard showing a chart with several colored lines representing the weights of each consumer class."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Consumer class weights over time.&lt;/div&gt;
  
&lt;/div&gt;

</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2020-07-06:/blog/2020/07/replacing_google_analytics_android.html</id>
    <title type="html">I replaced Google Analytics with a web server running on my phone</title>
    <published>2020-07-06T13:45:40Z</published>
    <updated>2020-07-06T13:45:40Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2020/07/replacing_google_analytics_android.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2020/07/replacing_google_analytics_android.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2020/simple_diagram.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/simple_diagram.png" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt; I built &lt;a href="https://github.com/lbrito1/android-analytics"&gt;android-analytics&lt;/a&gt;, a web analytics tracker running on my phone.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Say you run a blog, personal website, small-time business page or something of the sorts. Say you also want to keep an eye on how many visitors you’re getting.&lt;/p&gt;

&lt;p&gt;The first thing that most people think at this point is “Google Analytics”. It mostly works and is free. Its also hosted by Google, which makes it very easy to start using. There aren’t many competitors that bring those points to the table, so Google Analytics usually wins by WO at this point.&lt;/p&gt;

&lt;p&gt;I used to use Google Analytics to track this blog for those same reasons. But after finding out about &lt;a href="https://termux.com"&gt;Termux&lt;/a&gt; and writing &lt;a href="https://lbrito1.github.io/blog/2020/02/repurposing-android.html"&gt;this post&lt;/a&gt; about installing a web server on an Android phone, I started toying with the idea that I had this ARM-based, 2GB RAM, Linux-like device with Internet connectivity which must be more than enough for a simple webcounter-like application. After a few weeks of tinkering, here it is!&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id="table-of-contents"&gt;Table of Contents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
&lt;a href="#motivation"&gt;Motivation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href="#why-even-keep-anything"&gt;Why even keep anything?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="#and-then-there-is-the-data"&gt;And then there is the data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="#the-lack-of-competition"&gt;The (lack of) competition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
&lt;a href="#developing-android-analytics"&gt;Developing android-analytics&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href="#basis"&gt;Basis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="#first-iteration-sinatra-webapp"&gt;First iteration: Sinatra webapp&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="#second-iteration-nginx-log-parser"&gt;Second iteration: Nginx log parser&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="#third-iteration-adding-a-viewer"&gt;Third iteration: Adding a viewer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="#fourth-iteration-adding-an-installation-script"&gt;Fourth iteration: Adding an installation script&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="#final-architecture"&gt;Final architecture&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="motivation"&gt;Motivation&lt;/h2&gt;

&lt;h3 id="why-even-keep-anything"&gt;Why even keep anything?&lt;/h3&gt;

&lt;p&gt;Before going into this whole thing, there’s a very reasonable question to be answered: why do I even need to collect this data?&lt;/p&gt;

&lt;p&gt;The answer is simple: I really don’t, I just enjoy seeing it. Call it a &lt;a href="https://techcrunch.com/2011/07/30/vanity-metrics/"&gt;vanity metric&lt;/a&gt;, but I think its just &lt;em&gt;plain cool&lt;/em&gt; to know that someone half across the planet read something I wrote months ago (maybe it was just a crawler; I’ll take it either way).&lt;/p&gt;

&lt;p&gt;It should be no surprise, then, that Google Analytics always felt immensely overkill.&lt;/p&gt;

&lt;p&gt;Its heartwarming to know that some nerd from Bhutan read one of my posts in the wee hours of the morning, but that is pretty much all I’m interested in. I could care less about Acquisition Treemaps, Audience Cohort Analysis or Behavior Flow. I’m not making those up: they’re all real products available on Google Analytics. I have no idea of what any of those mean, yet I’m 100% sure I don’t need them.&lt;/p&gt;

&lt;div class="image-box"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/visitor_count.jpeg" target="_blank"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2020/visitor_count.jpeg" alt="Visitor counter from the 90s."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/visitor_count.jpeg" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Visitor counter from the 90s.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;What I wanted was closer to the late 90s’ visitor count GIF above (minus the embarrassment of publicity) than to the unsightly “Intersitial online advertising network conglomerate SEO dashboard” feeling of Google Analytics:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/ga.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/ga.png" alt="Google Analytics dashboard."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/ga.png" alt="Google Analytics dashboard."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Google Analytics dashboard.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;In short, I wanted to geek out, not do advertisement arbitrage.&lt;/p&gt;

&lt;h3 id="and-then-there-is-the-data"&gt;And then there is the data&lt;/h3&gt;

&lt;p&gt;As aforementioned, Google Analytics is great, free, &lt;em&gt;and hosted by Google&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;They keep your data. I have no idea of what they do with that data, or even what exactly it is that their tracker is sending to their servers (judging from the number of articles showing how to keep the payload below the cap of 8kb, it must be a lot).&lt;/p&gt;

&lt;div class="image-box"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/ga_payload.png" target="_blank"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2020/ga_payload.png" alt="Google search results for 'google analytics payload size is too large'. 642,000 results."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/ga_payload.png" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;That's a lot of results.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Apparently they often need over 8kb per request to feed their Lovecraftian “Audience Cohort Analysis” line of products. Fair enough, but I’m pretty sure that for my purposes, a several-kb payload is effectively using a sledgehammer to kill a fly.&lt;/p&gt;

&lt;p&gt;By using Google Analytics I was willfully sending Google who-knows-what kind of data designed to build up people’s advertising profile. The page views of my blog probably didn’t help Google too much in that aspect, sure, but the principle of the whole thing still bothered me enough to do something about it.&lt;/p&gt;

&lt;h3 id="the-lack-of-competition"&gt;The (lack of) competition&lt;/h3&gt;

&lt;p&gt;There are a lot of software similar to Google Analytics out there. The most prominent is probably &lt;a href="https://matomo.org/"&gt;Matomo&lt;/a&gt;, often &lt;a href="https://hn.algolia.com/?dateRange=all&amp;amp;page=0&amp;amp;prefix=true&amp;amp;query=matomo&amp;amp;sort=byPopularity&amp;amp;type=story"&gt;posted on Hacker News&lt;/a&gt;. It is free, open source and self-hosted (with cloud offerings for a monthly fee).&lt;/p&gt;

&lt;p&gt;I would happily use Matomo, but with it comes a conundrum:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Self-hosting implies I had to have some kind of publicly accessible Linux host, which would likely not be entirely free;&lt;/li&gt;
  &lt;li&gt;Cloud-hosting comes with a subscription fee.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those points are trivial if you’re running a lucrative business that &lt;em&gt;needs&lt;/em&gt; analytics, but paying for this service sounds ludicrous when all you want is simple visitor stats for a personal blog.&lt;/p&gt;

&lt;h2 id="developing-android-analytics"&gt;Developing android-analytics&lt;/h2&gt;

&lt;p&gt;These were the requirements I had for my tracker:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Has to run on an old Android phone I have lying around;&lt;/li&gt;
  &lt;li&gt;Has to work with Github Pages-hosted sites;&lt;/li&gt;
  &lt;li&gt;Has a per-page view count;&lt;/li&gt;
  &lt;li&gt;Nice to have: geo info.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These requirements are deceivingly simple, as I quickly learned.&lt;/p&gt;

&lt;p&gt;Termux makes it really easy to run many kinds of software on your Android phone, and &lt;a href="https://lbrito1.github.io/blog/2020/02/repurposing-android.html"&gt;I had already tinkered&lt;/a&gt; with web servers with Termux. For something as simple as a page view, this should be pretty straightforward.&lt;/p&gt;

&lt;p&gt;I had also already registered a dynamic DNS subdomain pointing to my phone, so it was ready to accept incoming traffic from the Internet.&lt;/p&gt;

&lt;p&gt;The first major roadblock I faced was getting my Android-hosted web server to communicate with Github Pages. After a couple of days of research, I finally learned that it is basically impossible to make a request from an HTTPS website (which Github Pages is) to an HTTP address (my Dynamic DNS’s subdomain). To summarize, you can make that work, but at the cost of having the client browser do something (like actively mark a “allow mixed content” checkbox somewhere in the browser’s flags/advanced options).&lt;/p&gt;

&lt;p&gt;This lead me to the excruciating path of obtaining and using a verified SSL certificate in my Android phone with a Dynamic DNS subdomain. This took me long enough to want to write a separate &lt;a href="https://lbrito1.github.io/blog/2020/06/free_https_home_server.html"&gt;blog post&lt;/a&gt; about it. The TLDR here is that it is entirely possible to get a verified SSL cert for a Dynamic DNS subdomain – all of it entirely for free. Depending on your ISP, you’ll have different choices of SSL challenges, but if you’re able to receive TCP requests on port &lt;code&gt;443&lt;/code&gt;, it is possible to get the certificate for free.&lt;/p&gt;

&lt;p&gt;Once I figured out the SSL thing, the rest was pretty much a breeze.&lt;/p&gt;

&lt;h3 id="fundamentals"&gt;Fundamentals&lt;/h3&gt;

&lt;p&gt;I tried out a few different ideas when developing this, but the overall architecture is always the same:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;JavaScript code in my tracked page calls the Android host;&lt;/li&gt;
  &lt;li&gt;Android host saves that information in a database;&lt;/li&gt;
  &lt;li&gt;Some graphical tool is used to parse that data into something viewable (charts etc).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="first-iteration-sinatra-webapp"&gt;First iteration: Sinatra webapp&lt;/h3&gt;

&lt;p&gt;I started with a &lt;a href="http://sinatrarb.com/"&gt;Sinatra&lt;/a&gt; webapp with a single &lt;code&gt;POST&lt;/code&gt; endpoint that would receive a request from the tracked page and immediately save it in a Postgres database. I used Nginx as a reverse-proxy that handled traffic before passing it to Sinatra.&lt;/p&gt;

&lt;p&gt;This approach had the merit of being simple to understand and reliable. Also, it worked.&lt;/p&gt;

&lt;p&gt;But after watching it work for a few days, I realized that the whole webapp part was superfluous. Nginx logs all accesses by default, and the logs contain all the information I need: what page was requested, at what time and from what IP. This lead naturally to the second iteration.&lt;/p&gt;

&lt;h3 id="second-iteration-nginx-log-parser"&gt;Second iteration: Nginx log parser&lt;/h3&gt;

&lt;p&gt;Nginx provides flexible, per-endpoint logs: logs are activated for the endpoint that I want (&lt;code&gt;/damn_fine_coffee&lt;/code&gt;) and deactivated for everything else. This is important because the Internet is full of crawlers that annoyingly hit the root path &lt;code&gt;/&lt;/code&gt;, which obviously shouldn’t count as a page view. As I learned, the web is also surprisingly full of smartypants trying to make their way into &lt;code&gt;/tp-link&lt;/code&gt;, &lt;code&gt;/admin&lt;/code&gt; and so on; I also wanted to just ignore those.&lt;/p&gt;

&lt;p&gt;The logs provided all the &lt;em&gt;data&lt;/em&gt; I needed, but I still needed to transform that &lt;em&gt;data&lt;/em&gt; into useful &lt;em&gt;information&lt;/em&gt;. I found out about &lt;a href="https://goaccess.io/"&gt;GoAccess&lt;/a&gt; on Hacker News, and, perhaps surprisingly, it worked out of the box with Termux:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/goaccess.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/goaccess.png" alt="GoAccess dashboard with my Android-hosted data."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/goaccess.png" alt="GoAccess dashboard with my Android-hosted data."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;GoAccess dashboard with my Android-hosted data.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;At this point I could settle for GoAccess, but it didn’t seem to provide any geo info, which I always thought would be a cool feature, so I kept working on my own tool.&lt;/p&gt;

&lt;p&gt;I configured Nginx to print CSV-like logs, and &lt;a href="https://github.com/lbrito1/android-analytics/blob/master/app/compiler.rb"&gt;wrote a parser&lt;/a&gt; that transforms those log entries into DB entries with geographic information provided by the excellent &lt;a href="https://github.com/alexreisner/geocoder"&gt;geocoder&lt;/a&gt; gem, and also annonymizes the request IPs using MD5 hashing. The final step was adding a cron entry to run the parser regularly.&lt;/p&gt;

&lt;p&gt;At this point I was getting regular traffic converted to rows in a Postgresql table. I still needed a more convenient way to look at the data, though.&lt;/p&gt;

&lt;h3 id="third-iteration-adding-a-viewer"&gt;Third iteration: Adding a viewer&lt;/h3&gt;

&lt;p&gt;I initially thought about using &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; as a visualization tool. Its free, easy to use, flexible and I was already familiar with it. Unfortunately Grafana doesn’t have binaries available for Termux (there’s an &lt;a href="https://github.com/termux/termux-packages/issues/4801"&gt;issue&lt;/a&gt; open in Termux’s repo requesting that), and I wasn’t feeling like trying to compile it manually.&lt;/p&gt;

&lt;p&gt;Thankfully I found the &lt;a href="https://github.com/ankane/blazer"&gt;blazer&lt;/a&gt; gem, which has a very similar concept compared with Grafana: you write SQL queries and it transforms them into charts. That was exactly what I was looking for. The downside is that it requires a full-fledged Rails application to run, but I was okay with that trade-off.&lt;/p&gt;

&lt;p&gt;Here’s how the data looks like right now:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/android-analytics-screenshot.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/android-analytics-screenshot.png" alt="blazer gem dashboard."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/android-analytics-screenshot.png" alt="blazer gem dashboard."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;blazer gem dashboard.&lt;/div&gt;
  
&lt;/div&gt;

&lt;h3 id="fourth-iteration-adding-an-installation-script"&gt;Fourth iteration: Adding an installation script&lt;/h3&gt;

&lt;p&gt;So far I was playing by ear; I knew more or less how to reinstall the project on a new device, but I knew that after some time my memory would fade and the process would become a painstaking trial-and-error mess.&lt;/p&gt;

&lt;p&gt;I first compiled all the steps needed for this to work in the repo’s README – it took a total of &lt;a href="https://github.com/lbrito1/android-analytics/commit/9487a54b37c727bdd60b7276469fc58a8fd0d47d#diff-04c6e90faac2675aa89e2176d2eec7d8"&gt;17 steps&lt;/a&gt; to get things running. Noticing that most of these steps could be automated, I wrote a &lt;a href="https://github.com/lbrito1/android-analytics/blob/master/bin/setup.sh"&gt;setup script&lt;/a&gt; that should do most of the work. I tested it in a separate Android device to make sure it works – hopefully it works for other people as well.&lt;/p&gt;

&lt;h3 id="final-architecture"&gt;Final architecture&lt;/h3&gt;

&lt;p&gt;When someone accesses one of my tracked pages, this is roughly what happens:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;JavaScript on that page calls my domain (provided for free by &lt;a href="https://www.duckdns.org/"&gt;DuckDNS&lt;/a&gt;);&lt;/li&gt;
  &lt;li&gt;DuckDNS translates that address to my router’s most recent IP;&lt;/li&gt;
  &lt;li&gt;My router receives that request and uses the NAT table to redirect it to my Android phone;&lt;/li&gt;
  &lt;li&gt;On Android, Nginx receives the request and either logs it if the request comes from the right place (my list of tracked pages), or does nothing otherwise;&lt;/li&gt;
  &lt;li&gt;A scheduled Cron job rotates Nginx logs and converts the “old” log into rows in a Postgresql table;&lt;/li&gt;
  &lt;li&gt;I open &lt;code&gt;&amp;lt;my-android-local-ip&amp;gt;:3000&lt;/code&gt; on my desktop’s browser and view the charts, maps etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This diagram shows those same steps, more or less:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/diagram.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/diagram.png" alt="android-analytics diagram."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/diagram.png" alt="android-analytics diagram."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;android-analytics diagram.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;I named the too (quite unimaginatively) android-analytics; code and set-up instructions are &lt;a href="https://github.com/lbrito1/android-analytics"&gt;available on Github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="august-2021-update"&gt;August 2021 Update&lt;/h3&gt;

&lt;p&gt;I managed to install Grafana on Termux by using &lt;a href="https://f-droid.org/en/packages/exa.lnx.a/"&gt;AnLinux&lt;/a&gt;; thus, the Viewer part of the project is no longer needed.&lt;/p&gt;

&lt;p&gt;Also, by using &lt;a href="https://ngrok.com/"&gt;Ngrok&lt;/a&gt; (free tier), the project now works if you’re behind CGNAT, which is my case. No need for dynamic DNS or port forwarding as well.&lt;/p&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I used the Google Analytics analogy because that’s the tool that most people are familiar with, and most people will immediately understand what this thing is about, which probably wouldn’t happen if instead of saying this was a “simple Google Analytics alternative”, I said it was a “log-based web analytics tool”.&lt;/p&gt;

&lt;p&gt;But saying this is a “Google Analytics replacement” is like saying that a bicycle is a replacement for a truck. Although they are both transportation modes, they’re different in every other aspect. The thing is: sometimes you really need a truck, but a lot of times you just need to get from point A to point B, and a bike is more than enough. In fact, it is probably &lt;em&gt;better&lt;/em&gt;: it is cheaper, easier to park and carry around, and has a smaller environmental footprint. This project is a bike: for some people, that’s all they will need.&lt;/p&gt;

&lt;p&gt;There’s absolutely no need to use a mammoth like Google Analytics for a personal blog or pet project. Its more than wasteful – you’re offering free data to Google in exchange for a fancy dashboard so you can play I’m-SEO-master-at-Adcorp-LLC. Someone has to keep the data, of course, but I’d argue that a decentralized approach is much safer and probably more ethical than data monopoly by a single huge advertising company.&lt;/p&gt;

&lt;p&gt;So what are the alternatives? There are a few competitors – we already discussed that in a &lt;a href="#the-lack-of-competition"&gt;previous section&lt;/a&gt;. But then we have all this processing power just lying around, free and unused; we might as well make better use of it. Smartphones have amazing processing, networking and storage capabilities, yet for many reasons they turn old very quickly, which translates to getting sold (in the best case); shoved into oblivion in our designated e-junk clutter drawer; or just discarded.&lt;/p&gt;

&lt;p&gt;It is just sad that we have these tiny slabs of processing power that could &lt;a href="https://www.realclearscience.com/articles/2019/07/02/your_mobile_phone_vs_apollo_11s_guidance_computer_111026.html"&gt;navigate Man to the Moon and back thousands of times over&lt;/a&gt;, and we can’t seem to quite find any better occupation for them other than sitting in a dusty drawer for years or getting trashed. That is why even if it takes a little extra effort, I’d rather repurpose and reuse something I already own than subscribe to the fanciest new PaaS.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2020-06-27:/blog/2020/06/free_https_home_server.html</id>
    <title type="html">Setting up a free HTTPS home server</title>
    <published>2020-06-27T22:48:21Z</published>
    <updated>2020-06-27T22:48:21Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2020/06/free_https_home_server.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2020/06/free_https_home_server.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2020/cool-background.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/cool-background.png" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Try searching for “free dynamic dns https”, “free domain with SSL” or anything similar. There won’t be a lot of meaningful results. Sure, some of the results are pretty close, like &lt;a href="https://www.freecodecamp.org/news/free-https-c051ca570324/"&gt;this guide&lt;/a&gt; on how to get free SSL certification from Cloudflare, or &lt;a href="https://medium.com/@jeremygale/how-to-set-up-a-free-dynamic-hostname-with-ssl-cert-using-google-domains-58929fdfbb7a"&gt;this one&lt;/a&gt; on setting up a free dynamic hostname with SSL, but they all assume you &lt;em&gt;already own a domain&lt;/em&gt;. If you’re looking for a completely free domain that you can use for your personal web server that also has verified SSL, there are very few results.&lt;/p&gt;

&lt;p&gt;But why was I even looking for this?&lt;/p&gt;

&lt;p&gt;I’m working on a side project. It has a web server that communicates with a static web page hosted on Github Pages. There are a lot of ways of setting that up; in my particular case, I have a local (as in in my house) HTTP web server accepting traffic on a non-standard port (port &lt;code&gt;80&lt;/code&gt; is blocked by my ISP &lt;a href="https://www.reddit.com/r/InternetBrasil/comments/e9v5o0/abertura_das_portas_80_e_443_na_claronet/"&gt;for commercial reasons&lt;/a&gt; – this detail is of paramount importance, but more on that later). It is accessible through my external IP (which is dynamic), which can be mapped to a dynamic DNS domain.&lt;/p&gt;

&lt;p&gt;I wanted to run a simple API on the web server and access it through static pages (like this blog) hosted on Github Pages (which has a verified SSL certificate). &lt;a href="https://stackoverflow.com/questions/62378047/is-it-possible-to-make-a-cross-domain-javascript-request-to-http-from-https"&gt;I asked the Internet&lt;/a&gt; if it is possible to call from a SSL-verified page (in JavaScript) a different server that does not have a verified SSL certificate (that is, my aforementioned webapp running in my home server). It isn’t, so the conclusion was that I needed somehow to get a verified SSL certificate for my dynamic DNS domain.&lt;/p&gt;

&lt;p&gt;Having no idea whether this was possible, I started to research.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id="setting-up-dynamic-dns"&gt;Setting up Dynamic DNS&lt;/h2&gt;

&lt;p&gt;Most ISPs provide dynamic IP addresses for their residential customers, while static IP addresses are usually reserved to the “commercial” or “business” tier. That means your public IP address changes (usually every &lt;a href="https://vicimediainc.com/often-ip-addresses-change/"&gt;14 days&lt;/a&gt;), so DNS servers will have to keep track of your changing IP somehow. That kind of service is called Dynamic DNS, or DDNS for short.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://free-for.dev/#/?id=dns"&gt;Several companies&lt;/a&gt; provide DDNS service for free. Some of them also provide a free subdomain, which is useful if you don’t own a domain yourself (I don’t). I’ve tried out most of the free DDNS providers, the most prominent seeming to be Hurricane Electric, No-ip, Dynu and DuckDNS. If you’re up for it there are even several blog posts out there explaining &lt;a href="https://blog.heckel.io/2016/12/31/your-own-dynamic-dns-server-powerdns-mysql/"&gt;how to set up your own dynamic DNS server&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I wasn’t feeling too adventurous so I decided to set up shop with DuckDNS. It is really easy to set up, comes with a great HTTP API for updating the domain’s TXT, provides free subdomains that don’t expire (No-ip for instance has subdomains that expire after 30 days), and has a valid SSL certificate. They have a page &lt;a href="https://www.duckdns.org/install.jsp"&gt;explaining how to set up the actual DDNS service&lt;/a&gt;, so I’ll skip that.&lt;/p&gt;

&lt;h3 id="caveat-carrier-grade-nat"&gt;Caveat: carrier-grade NAT&lt;/h3&gt;

&lt;p&gt;One big potential problem in the DDNS setup is whether you’re behind a &lt;a href="https://www.wikiwand.com/en/Carrier-grade_NAT"&gt;carrier-grade NAT (CGNAT)&lt;/a&gt;, which some ISPs unfortunately do. In short, being in a CGNAT boils down to not having a public IP address – you’re part of your ISP’s private network, and your router’s “public” IP address is actually a private IP address within that private network, which the ISP translates to and from the Internet.&lt;/p&gt;

&lt;p&gt;CGNATs suck, and it essentially &lt;a href="https://www.reddit.com/r/HomeNetworking/comments/6ahcp6/rtn66u_isp_changed_to_cgnat_broke_ddns/"&gt;makes using DDNS impossible&lt;/a&gt;. You can find out if you’re behind a CGNAT by comparing your WAN IP address (displayed in the router admin page) and your public IP. If they differ, you’re probably behind a CGNAT&lt;/p&gt;

&lt;h2 id="setting-up-a-verified-ssl"&gt;Setting up a verified SSL&lt;/h2&gt;

&lt;p&gt;I had set up the dynamic DNS service, and the next step was finding out if it was even possible to obtain a free valid SSL certificate for my subdomain.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://letsencrypt.org/getting-started/"&gt;Let’s Encrypt&lt;/a&gt; provides free valid SSL certificates, which are usually obtained by using &lt;a href="https://certbot.eff.org/"&gt;Certbot&lt;/a&gt;, a handy software that will handle most of the complicated SSL verification process you. There are &lt;a href="https://letsencrypt.org/docs/client-options/"&gt;several other&lt;/a&gt; alternative tools that implement the same protocol used by Let’s Encrypt, but I really recommend using Certbot – it has much better out-of-the-box functionality than all the other tools I tried out, and the community is much bigger. The only caveat I could find is that you need &lt;code&gt;sudo&lt;/code&gt; access to use it properly.&lt;/p&gt;

&lt;p&gt;One thing I’d wish someone had told me before I spent hours looking for alternatives to Certbot is that &lt;strong&gt;it doesn’t have to be executed in the host that is ultimately going to obtain the SSL certificate&lt;/strong&gt;. This might be a crucial bit of information if you can’t run as root on the actual host that will obtain the SSL certificate, which was my case. It is perfectly fine to run Certbot on a separate computer, obtain the SSL certificates and then &lt;code&gt;scp&lt;/code&gt; them to the correct host.&lt;/p&gt;

&lt;p&gt;Now, as I mentioned, my ISP blocks incoming traffic to port &lt;code&gt;80&lt;/code&gt; for their residential customers. This is relevant because Let’s Encrypt uses by default the &lt;strong&gt;HTTP-01 challenge&lt;/strong&gt; in the SSL verification process, and it requires the ports &lt;code&gt;80&lt;/code&gt; and &lt;code&gt;443&lt;/code&gt; to be open. However, LE also offers the alternative &lt;strong&gt;&lt;a href="https://letsencrypt.org/docs/challenge-types/"&gt;DNS-01 challenge&lt;/a&gt;&lt;/strong&gt; which &lt;strong&gt;does not&lt;/strong&gt; require those ports to be open (but requires the ability to update TXT domain records, which not all DDNS service providers allow – No-ip, for instance, does not). I happened to find out about this by reading &lt;a href="https://www.splitbrain.org/blog/2017-08/10-homeassistant_duckdns_letsencrypt"&gt;this very helpful post&lt;/a&gt; from someone in a similar predicament (home server, port &lt;code&gt;80&lt;/code&gt; not available) saying he used this alternative challenge successfully with DuckDNS (thank you!). In &lt;a href="https://serverfault.com/a/812038/578968"&gt;this Server Fault answer&lt;/a&gt;, the poster explains how to use Certbot with the DNS-01 challenge (thank you!).&lt;/p&gt;

&lt;h3 id="running-certbot-with-dns-01-and-duckdns"&gt;Running Certbot with DNS-01 and DuckDNS&lt;/h3&gt;

&lt;p&gt;DNS-01 works by confirming that you can modify the DNS TXT record of your domain.&lt;/p&gt;

&lt;p&gt;Here’s the command to start SSL verification with Certbot using DNS-01 and a DuckDNS subdomain, and the expected output:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;sudo &lt;/span&gt;certbot &lt;span class="nt"&gt;-d&lt;/span&gt;  my-subdomain.duckdns.org &lt;span class="nt"&gt;--manual&lt;/span&gt; &lt;span class="nt"&gt;--preferred-challenges&lt;/span&gt; dns certonly

Saving debug log to /var/log/letsencrypt/letsencrypt.log
Plugins selected: Authenticator manual, Installer None
Obtaining a new certificate
Performing the following challenges:
dns-01 challenge &lt;span class="k"&gt;for &lt;/span&gt;my-subdomain.duckdns.org

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
NOTE: The IP of this machine will be publicly logged as having requested this
certificate. If you&lt;span class="s1"&gt;'re running certbot in manual mode on a machine that is not
your server, please ensure you'&lt;/span&gt;re okay with that.

Are you OK with your IP being logged?
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
&lt;span class="o"&gt;(&lt;/span&gt;Y&lt;span class="o"&gt;)&lt;/span&gt;es/&lt;span class="o"&gt;(&lt;/span&gt;N&lt;span class="o"&gt;)&lt;/span&gt;o: Y

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Please deploy a DNS TXT record under the name
_acme-challenge.my-subdomain.duckdns.org with the following value:

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Before continuing, verify the record is deployed.
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Press Enter to Continue&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At this point you have to do as the program says: update the DNS TXT record. Thankfully, this is exceedingly easy to do with DuckDNS (see their &lt;a href="https://www.duckdns.org/spec.jsp"&gt;spec page&lt;/a&gt; for instructions).&lt;/p&gt;

&lt;p&gt;You can verify that the TXT was updated by running &lt;code&gt;dig&lt;/code&gt;:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;dig my-subdomain.duckdns.org TXT

&lt;span class="p"&gt;;&lt;/span&gt; &amp;lt;&amp;lt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; DiG 9.11.3-1ubuntu1.12-Ubuntu &amp;lt;&amp;lt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; my-subdomain.duckdns.org TXT
&lt;span class="p"&gt;;;&lt;/span&gt; global options: +cmd
&lt;span class="p"&gt;;;&lt;/span&gt; Got answer:
&lt;span class="p"&gt;;;&lt;/span&gt; -&amp;gt;&amp;gt;HEADER&lt;span class="o"&gt;&amp;lt;&amp;lt;-&lt;/span&gt; &lt;span class="no"&gt;opcode&lt;/span&gt;&lt;span class="sh"&gt;: QUERY, status: NOERROR, id: 21765
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 65494
;; QUESTION SECTION:
;my-subdomain.duckdns.org. IN  TXT

;; ANSWER SECTION:
my-subdomain.duckdns.org. 59 IN  TXT "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

;; Query time: 335 msec
;; SERVER: 127.0.0.53#53(127.0.0.53)
;; WHEN: Mon Jun 15 18:50:41 -03 2020
;; MSG SIZE  rcvd: 114&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once you confirmed the TXT value, the remainder of Certbot’s output should be this success message:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;Waiting &lt;span class="k"&gt;for &lt;/span&gt;verification...
Cleaning up challenges

IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at:
   /etc/letsencrypt/live/my-subdomain.duckdns.org/fullchain.pem
   Your key file has been saved at:
   /etc/letsencrypt/live/my-subdomain.duckdns.org/privkey.pem
   Your cert will expire on 2020-09-13. To obtain a new or tweaked
   version of this certificate &lt;span class="k"&gt;in &lt;/span&gt;the future, simply run certbot
   again. To non-interactively renew &lt;span class="k"&gt;*&lt;/span&gt;all&lt;span class="k"&gt;*&lt;/span&gt; of your certificates, run
   &lt;span class="s2"&gt;"certbot renew"&lt;/span&gt;
 - If you like Certbot, please consider supporting our work by:

   Donating to ISRG / Let&lt;span class="s1"&gt;'s Encrypt:   https://letsencrypt.org/donate
   Donating to EFF:                    https://eff.org/donate-le&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All set! You now have a valid SSL certificate. You’ll still need to place it in the right place, which will vary depending on what web server you’re using. For example, if you’re using Nginx, the configuration file might look something like this:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code&gt;
server {
  ssl_certificate /path/to/fullchain.pem;
  ssl_certificate_key /path/to/privkey.pem;
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;There’s quite a lot of shady-looking websites out there offering for a monthly fee the exact same thing as I just wrote about. When researching this, not knowing too much about most of these topics, I was almost fooled into accepting that this just couldn’t be done for free for some unknown technical reason. There &lt;em&gt;had&lt;/em&gt; to be a reason why there were no Google results for this – maybe my case was too specific, or maybe other people are less cheap than I am and just pay for a domain and get the SSL stuff for free.&lt;/p&gt;

&lt;p&gt;I still have no good explanation as to why the kind of guide I just wrote above didn’t show up in my research. Maybe people don’t care about home servers, or maybe I’m just not too good at searching (probably both). In any case, hopefully this post will make it clear that setting up a DDNS subdomain with SSL for free is not only possible, but really not that complicated.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2020-05-30:/blog/2020/05/communication.html</id>
    <title type="html">Communication tips for remote developers</title>
    <published>2020-05-30T15:16:00Z</published>
    <updated>2020-05-30T15:16:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2020/05/communication.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2020/05/communication.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2020/bridge.jpg" alt="San Francisco Bay bridge."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/bridge.jpg" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;We're all remote -- for now.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Communicating well with your co-workers and managers is supremely important to a software developer, and even more so for the remote one. With a lot more remote workers due to the COVID-19 pandemic, this topic became a lot more relevant.&lt;/p&gt;

&lt;p&gt;I’ve seen people hint at this more than a few times over the years, but I didn’t really “get it” until I started working as a fully remote engineer. I also find it important to understand not only &lt;em&gt;what&lt;/em&gt; we should be doing to achieve efficient communication, but also &lt;em&gt;why&lt;/em&gt; we should be doing those things in those ways.&lt;/p&gt;

&lt;p&gt;To me, the single most important thing to keep in mind is that people’s mental resources: time, attention span, etc, like yours, are limited.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;That might be obvious, but different management styles might make it seem otherwise – a more hands-on manager might kind of seem like he is acutely aware of what you’re doing all the time, but that is hardly ever the case. Managing styles aside, managers are, after all, humans like the rest of us and have limited time and resources. They can’t possibly have the same insight into each task as the respective engineers working on them.&lt;/p&gt;

&lt;p&gt;The corollary to that is that it is your job to keep managers in the loop, providing the right amount of information at the right time through the right channel (text, video call, presentation…). Just like any other skill, this is something you can learn over time.&lt;/p&gt;

&lt;p&gt;Similar reasoning applies to co-workers: people are usually deeply involved in whatever it is they’re doing, so they won’t usually know too many details about what you or other co-workers are doing all the time.&lt;/p&gt;

&lt;p&gt;A lot of the things you need to do are fairly obvious and well-known: be clear about what you’re saying, keep communicating frequently, etc. Other things aren’t too obvious (at least to me, that is) and are worth sharing in this short blog post.&lt;/p&gt;

&lt;h2 id="dont-write-a-novel"&gt;Don’t write a novel&lt;/h2&gt;

&lt;p&gt;Reading is hard. People’s availability and attention span vary. Try to get your point across with the least words as possible.&lt;/p&gt;

&lt;p&gt;If I’m writing an issue update, pull request, or other technical information, I usually start with a more winding text and then prune as much of it as I can. This can be really simple, like changing “According to #85748, the problem I described started when…” to “The problem began when … (#85748)”.&lt;/p&gt;

&lt;p&gt;This can’t be done at the expense of clarity, though. It is preferable to write or say “I think option B is the way to go” than an ambiguous “sounds good”.&lt;/p&gt;

&lt;h2 id="manage-expectations"&gt;Manage expectations&lt;/h2&gt;

&lt;p&gt;People don’t like to feel disappointed. To avoid unfulfilled expectations, it is your job to make sure those expectations stay realistic – the more so when the task evolves or unravels into something much more complicated than people originally expected.&lt;/p&gt;

&lt;p&gt;As we all know, there’s a lot of uncertainty in this job. Something that seems easy might actually be super easy or might be very hard. Unexpected difficulties are expected, and most people are fine with that &lt;strong&gt;as long as they also think that those problems are actually problems.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;That’s a huge caveat: if you’re unable to convince other people about the seriousness of the unexpected problems you’re facing, you might as well not say anything at all about them. People usually only believe what they understand, and it is your job to properly communicate that to people less involved in the task than you are.&lt;/p&gt;

&lt;h2 id="tailor-to-the-audience"&gt;Tailor to the audience&lt;/h2&gt;

&lt;p&gt;Different people use different lingo to express the same things. Sales people will use different terms than engineering people.&lt;/p&gt;

&lt;p&gt;Adjusting your language to the audience isn’t just about replacing technical words with other words, though, but also about cropping the information in the right way.&lt;/p&gt;

&lt;p&gt;Excess information that isn’t relevant to the point you want to get across generates noise and confusion. This might be seen as a broader definition of the first topic: if you can manage to get your point across with less information (whatever that is: spreadsheets, images, etc), then that is certainly desirable. If the person you’re talking or presenting to is only interested in 1 of the 3 columns of a spreadsheet, although the other 2 might be insightful to you, you should probably refrain from showing them at that moment.&lt;/p&gt;

&lt;p&gt;And that is the note I’m ending this post on. These three things are probably obvious or second nature to a lot of people, but at least to me, it took a few years of remote work to fully appreciate them. Hopefully this post can be helpful to other like-minded developers.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2020-05-16:/blog/2020/05/figuring_out_nvidia_x_linux.html</id>
    <title type="html">Figuring out the Nvidia x Linux puzzle</title>
    <published>2020-05-16T19:48:00Z</published>
    <updated>2020-05-16T19:48:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2020/05/figuring_out_nvidia_x_linux.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2020/05/figuring_out_nvidia_x_linux.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2020/power.png" alt="Ubuntu power consumption chart."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/power.png" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Ubuntu's power rate over time.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;I’ve struggled with some kind of problem with Nvidia graphics cards on Linux since forever.&lt;/p&gt;

&lt;p&gt;Most commonly, an external monitor wouldn’t work or the dedicated card would refuse to power off when it should.&lt;/p&gt;

&lt;p&gt;The latter problem – a power-hogging discrete Nvidia card not turning off when it isn’t needed, specifically in &lt;a href="https://www.wikiwand.com/en/Nvidia_Optimus"&gt;Optimus&lt;/a&gt;-enabled laptops – has consistently haunted me throughout the years. At least in my experience, this problem is in that sweet spot of things that are definitively annoying and kind of inconvenient, but complicated enough not to be worth the several work-hours needed to definitively solve it.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;I know that I’m not alone here, as other people over the internet have said things like &lt;em&gt;&lt;a href="https://forum.manjaro.org/t/solved-bumblebee-issues-with-bbswitch/70137"&gt;“I’ve been pulling my hair out for the past few hours trying to configure my graphics drivers on my laptop”&lt;/a&gt;&lt;/em&gt;. I’ve also not been a total sloth about this: although I have tried many times in the past to fix this, I’ve consistently found myself thinking “okay, &lt;em&gt;now&lt;/em&gt; this is fixed”, only to a few hours/days later notice that my laptop battery was drained in an hour and the problem was back. I actually re-wrote a significant part of this post because when I thought I was finished, my Nvidia card started turning on again and I had to do more research.&lt;/p&gt;

&lt;p&gt;Taking advantage of the extra time in my hands due to the Covid-19 city-wide lockdown, I decided to persistently look for a solution to this issue. This guide is just a documentation of this process. I use Ubuntu, but similar steps should be valid with whatever distro you’re using. Also, some or many of the steps might not actually be necessary - they’re just what happened to finally work in my case.&lt;/p&gt;

&lt;h3 id="install-the-proprietary-nvidia-drivers"&gt;1. Install the proprietary Nvidia drivers&lt;/h3&gt;

&lt;p&gt;Ubuntu uses the open-source Nouveau driver for Nvidia cards, which doesn’t play well with Optimus-enabled laptops. Let’s install the proprietary Nvidia driver.&lt;/p&gt;

&lt;p&gt;First, find out what’s the recommended Nvidia driver:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;ubuntu-drivers devices

&lt;span class="o"&gt;==&lt;/span&gt; /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 &lt;span class="o"&gt;==&lt;/span&gt;
modalias : pci:v000010DEd00002191sv00001462sd00001274bc03sc00i00
vendor   : NVIDIA Corporation
driver   : nvidia-driver-435 - distro non-free
driver   : nvidia-driver-440 - distro non-free recommended
driver   : xserver-xorg-video-nouveau - distro free &lt;span class="nb"&gt;builtin&lt;/span&gt;

&lt;span class="o"&gt;==&lt;/span&gt; /sys/devices/pci0000:00/0000:00:14.3 &lt;span class="o"&gt;==&lt;/span&gt;
modalias : pci:v00008086d0000A370sv00008086sd00000034bc02sc80i00
vendor   : Intel Corporation
manual_install: True
driver   : backport-iwlwifi-dkms - distro free&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then install it:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;sudo &lt;/span&gt;apt &lt;span class="nb"&gt;install &lt;/span&gt;nvidia-440&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Another option is to pick the driver in the Additional Drivers tab of the &lt;code&gt;Softwares &amp;amp; Updates&lt;/code&gt; tool:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/2020-05-16-05-04.nvidia.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/2020-05-16-05-04.nvidia.png" alt="Nvidia proprietary driver option in Ubuntu's Additional Drivers menu."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/2020-05-16-05-04.nvidia.png" alt="Nvidia proprietary driver option in Ubuntu's Additional Drivers menu."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Nvidia proprietary driver option in Ubuntu's Additional Drivers menu.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Nvidia’s proprietary driver lets you choose if you want to use the dedicated or integrated GPU, which you can try setting:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/nvidia-setting.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/nvidia-setting.png" alt="Nvidia proprietary driver's GPU selection menu."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/nvidia-setting.png" alt="Nvidia proprietary driver's GPU selection menu."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Nvidia proprietary driver's GPU selection menu.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Now if you’re lucky this might be enough. Check the power usag using Ubuntu’s &lt;code&gt;Power Statistics&lt;/code&gt; tool or &lt;code&gt;powertop&lt;/code&gt;: if the Nvidia card is successfully turned off, then typical power usage is somewhere between 8-14W. If, like me, this changed nothing in your power usage, read on.&lt;/p&gt;

&lt;h3 id="install-and-configure-bbswitch"&gt;2. Install and configure bbswitch&lt;/h3&gt;

&lt;p&gt;Although Nvidia’s proprietary driver allows selecting between integrated and dedicated cards, in my experience that setting has had no effect at all, with both cards always being powered on anyway.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bbswitch&lt;/code&gt; is a tool that allows you to select which card you want your system to use. Ubuntu has the bbswitch-dkms package available:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;sudo &lt;/span&gt;apt &lt;span class="nb"&gt;install &lt;/span&gt;bbswitch-dkms&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then configure it to always turn off the discrete card by creating the following file:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cat&lt;/span&gt; /etc/modprobe.d/bbswitch.conf
options bbswitch &lt;span class="nv"&gt;load_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="blacklist-nouveau-driver"&gt;3. Blacklist Nouveau driver&lt;/h3&gt;

&lt;p&gt;According to &lt;a href="https://askubuntu.com/a/1044095/463850"&gt;this Stackoverflow answer&lt;/a&gt;, there seem to be at least a couple of bugs that result in Ubuntu trying to load the Nouveau module even if you’re using a proprietary Nvidia driver. When that happens, the discrete Nvidia GPU turns on and starts hogging a lot of power.&lt;/p&gt;

&lt;p&gt;Blacklisting the Nouveau module solved this issue for me:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;sudo &lt;/span&gt;bash &lt;span class="nt"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;"echo blacklist nouveau &amp;gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf"&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;sudo &lt;/span&gt;bash &lt;span class="nt"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;"echo options nouveau modeset=0 &amp;gt;&amp;gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf"&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Restart and confirm that the right driver is loaded:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;gpu-manager | &lt;span class="nb"&gt;grep &lt;/span&gt;nouveau
Is nouveau loaded? no
Is nouveau blacklisted? &lt;span class="nb"&gt;yes&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="blacklist-some-nvidia-modules"&gt;4. Blacklist some Nvidia modules&lt;/h3&gt;

&lt;p&gt;Even after the above, my system kept turning on the nvidia card seemingly at random. I found &lt;a href="https://github.com/Bumblebee-Project/Bumblebee/issues/951"&gt;this post&lt;/a&gt; in the Bumblebee issue tracker to be extremely helpful:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“bumblebee can turn the nvidia card off when it starts, but as soon as the nvidia module is loaded, it loads nvidia_drm, which links to drm_kms_helper and then bumblebee can’t remove the nvidia modules. This means that bumblebee can’t turn off the nvidia card when optirun terminates. To fix this, I added “alias nvidia_drm off” and “alias nvidia_modeset off” to my conf file in /etc/modprobe.d.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the file created by the OP:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cat&lt;/span&gt; /etc/modprobe.d/nvidia.conf

blacklist nvidia
blacklist nvidia_drm
blacklist nvidia_modeset
&lt;span class="nb"&gt;alias &lt;/span&gt;nvidia_drm off
&lt;span class="nb"&gt;alias &lt;/span&gt;nvidia_modeset off&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After creating this file and restarting, my system was finally using only the Intel integrated card. Hopefully this time it’ll stay that way.&lt;/p&gt;

&lt;h3 id="results"&gt;Results&lt;/h3&gt;

&lt;p&gt;Here’s a chart of my laptop’s power rate:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/power.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/power.png" alt="Ubuntu power consumption chart."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/power.png" alt="Ubuntu power consumption chart."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Ubuntu's power rate over time.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Using the integrated Intel GPU, the rate fluctuates around 10W. When the Nvidia card kicks in, which is what was going on around the middle of the chart, it jumps to 40W+.&lt;/p&gt;

&lt;h3 id="references"&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=""&gt;https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-18-04-bionic-beaver-linux&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""&gt;https://github.com/Bumblebee-Project/bbswitch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""&gt;https://github.com/Bumblebee-Project/Bumblebee/issues/951&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""&gt;https://turlucode.com/optimus-bbswitch-on-ubuntu-18-04/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2020-02-05:/blog/2020/02/repurposing-android.html</id>
    <title type="html">Repurposing an old Android phone as a Ruby web server</title>
    <published>2020-02-05T12:24:41Z</published>
    <updated>2020-02-05T12:24:41Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2020/02/repurposing-android.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2020/02/repurposing-android.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2020/old-android.jpg" alt="Old smartphones on a desk."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/old-android.jpg" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;CC-BY Carlos Varela, https://www.flickr.com/photos/c32/7755470064&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Do you have an old Android phone? Sure you do! There’s a mind-blowing amount of electronic waste of all kinds, and with the average person in developed countries &lt;a href="https://www.cnbc.com/2019/05/17/smartphone-users-are-waiting-longer-before-upgrading-heres-why.html"&gt;discarding their phones every couple of years&lt;/a&gt;, discarded smartphones are probably one of the most common forms of e-waste.&lt;/p&gt;

&lt;p&gt;I had an old Motorola G5 Cedric gathering dust, so I decided to do something with it – it is now running a Puma web server with a simple Sinatra webapp.&lt;/p&gt;

&lt;p&gt;Now, before going any further, you might be thinking: what is the real, practical use of all this? An old Android phone probably isn’t going to have a stellar performance, but neither do those &lt;code&gt;t2.nano&lt;/code&gt;s, honestly. I’m yet to deploy any “real” code on an Android, but even the cheaper and older phones do commonly have quad-core or even octa-core CPUs, and at least 2GB RAM, so at least in theory a phone &lt;em&gt;should&lt;/em&gt; be close – ballpark, at least – to the most modest cloud IaaS offers our there (&lt;code&gt;t2.nano&lt;/code&gt; has 512MB for instance). Of course, a phone has an ARM processor while IaaS usually are x86; memory management is entirely different as well, but still – we’re talking ballpark estimates here.&lt;/p&gt;

&lt;p&gt;Anyway, this is a short tutorial on how to repurpose an Android device as a web server – or any number of different things, really.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id="install-termux"&gt;1. Install Termux&lt;/h2&gt;

&lt;p&gt;First of all we need a Linux environment in our phone. Termux is a terminal emulator and Linux environment for Android. It’s available on Google Play Store. No additional configuration is needed after installation.&lt;/p&gt;

&lt;h2 id="set-up-ssh"&gt;2. Set up SSH&lt;/h2&gt;

&lt;p&gt;You won’t want to type a lot of commands into a tiny touchscreen, so let’s set up ssh so that we can log into Termux remotely.&lt;/p&gt;

&lt;p&gt;There are &lt;a href="https://wiki.termux.com/wiki/Remote_Access"&gt;several ways&lt;/a&gt; of doing this, but I’ve found that the easiest way is through a software called &lt;strong&gt;Dropbear&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Run this on Android:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;pkg upgrade
pkg &lt;span class="nb"&gt;install &lt;/span&gt;dropbear&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can use password-based authentication or public key authentication. You should use key-based authentication, but for testing purposes password-based is easiest. Run this on Android:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Run this on Android:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;passwd
New password:
Retype new password:
New password was successfully set.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Bonus points:&lt;/strong&gt; install a terminal multiplexer like &lt;code&gt;tmux&lt;/code&gt; or &lt;code&gt;screen&lt;/code&gt;. This will make your life much easier when running stuff via ssh:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;pkg &lt;span class="nb"&gt;install &lt;/span&gt;tmux&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now go ahead and test the connection on your desktop:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;ssh android-ip-address &lt;span class="nt"&gt;-p&lt;/span&gt; 8022&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="set-up-static-ip-address-on-android"&gt;3. Set up static IP address on Android&lt;/h2&gt;

&lt;p&gt;Go to wifi settings, disable DHCP and assign an IP address for your phone.&lt;/p&gt;

&lt;p&gt;This is necessary so that your router won’t assign a new IP address to your phone every few hours/days, which would make configuration a lot harder.&lt;/p&gt;

&lt;h2 id="install-ruby-bundler-sinatra-and-puma"&gt;4. Install Ruby, Bundler, Sinatra and Puma&lt;/h2&gt;

&lt;p&gt;Sinatra is a lightweight web application framework, and Puma is a web server.&lt;/p&gt;

&lt;p&gt;Ruby is, well Ruby!&lt;/p&gt;

&lt;p&gt;Of course, Sinatra and Puma are just suggestions – you could even use full-blown Rails on your phone, as described in &lt;a href="https://mbobin.me/ruby/2017/02/25/ruby-on-rails-on-android.html"&gt;this neat tutorial&lt;/a&gt;. Just &lt;a href="https://devcenter.heroku.com/articles/ruby-default-web-server#why-not-webrick"&gt;don’t use WEBRick&lt;/a&gt;, the default Rails web server in development – it is single-process, single-threaded and thus not suitable for production environments (it is fine for small experiments though).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Run this on Android:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;pkg &lt;span class="nb"&gt;install &lt;/span&gt;ruby
gem &lt;span class="nb"&gt;install &lt;/span&gt;sinatra puma&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="install-nginx"&gt;Install nginx&lt;/h2&gt;

&lt;p&gt;nginx is a web server, reverse-proxy and load balancer. Although most useful in multi-server setups where it is used to distribute requests among different instances, nginx is also a good idea in our setup because of the embedded DDoS protection and static file serving that it provides.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Run this on Android:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-bash"&gt;pkg &lt;span class="nb"&gt;install &lt;/span&gt;nginx&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now the slightly tricky part is configuring nginx to work with Puma. &lt;a href="https://gist.github.com/ctalkington/4448153"&gt;This gist&lt;/a&gt; is a pretty good start – copy &amp;amp; paste &lt;code&gt;nginx.conf&lt;/code&gt; and change &lt;code&gt;appdir&lt;/code&gt; to your webapp’s root dir. In my case, for example, that would be &lt;code&gt;/data/data/com.termux/files/home/android-sinatra&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id="set-up-port-forwarding"&gt;Set up port forwarding&lt;/h2&gt;

&lt;p&gt;You probably want your web server to be accessible through the internet, so you’ll have to set up port forwarding in your router to redirect incoming requests to your public IP address to your brand new Android web server.&lt;/p&gt;

&lt;p&gt;How exactly to do this varies depending on your router. &lt;a href="https://www.noip.com/support/knowledgebase/general-port-forwarding-guide/"&gt;Here’s&lt;/a&gt; a pretty good tutorial to get you started.&lt;/p&gt;

&lt;h2 id="configure-a-dynamic-dns"&gt;Configure a dynamic dns&lt;/h2&gt;

&lt;p&gt;Most people have dynamic public IP addresses. In these cases it is useful to set up a dynamic dns (DDNS) service, which provides you with a static domain name that redirects automatically to whatever your public IP address is at that moment.&lt;/p&gt;

&lt;p&gt;There are few free services that provide DDNS nowadays; I’m using &lt;a href="https://www.noip.com/"&gt;no-ip&lt;/a&gt; and it has been okay so far. You do have to “renew” your domain every month though.&lt;/p&gt;

&lt;p&gt;After setting up a DDNS, you’ll have to configure your router as well so that it periodically notifies the DDNS service with your current IP address. Again, how exactly to do this depends on your router model.&lt;/p&gt;

&lt;h2 id="hello-world"&gt;Hello world!&lt;/h2&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/android-web-server.jpg" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/android-web-server.jpg" alt="Puma and nginx running on a Motorola G5."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/android-web-server.jpg" alt="Puma and nginx running on a Motorola G5."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Puma and nginx running on a Motorola G5.&lt;/div&gt;
  
&lt;/div&gt;

&lt;h2 id="under-siege"&gt;Under siege&lt;/h2&gt;

&lt;p&gt;You can simulate real-world usage through &lt;a href="https://www.joedog.org/siege-home/"&gt;&lt;code&gt;siege&lt;/code&gt;&lt;/a&gt;, a http load testing software. Here’s a screenshot of &lt;code&gt;siege&lt;/code&gt; running on my setup with 3 concurrent users (real tests would use bigger numbers):&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/siege.jpg" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/siege.jpg" alt="Screenshot of siege running on a terminal."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/siege.jpg" alt="Screenshot of siege running on a terminal."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;siege running in the foreground; nginx logs and top on remote (android) running in the background terminals.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;The numbers in that screenshot don’t matter much because our webapp was serving a simple 100-char response with a timestamp, but it is enough to at least know that the server can handle a few concurrent users.&lt;/p&gt;

&lt;h2 id="epilogue-safety"&gt;Epilogue: safety&lt;/h2&gt;

&lt;p&gt;If you’ve watched &lt;a href="https://en.wikipedia.org/wiki/Mr._Robot"&gt;Mr Robot&lt;/a&gt;, you know that the internet can be a dangerous place. That is a lot more true if you have a web server open to the internet.&lt;/p&gt;

&lt;p&gt;Within a few hours of opening up the server, it was already being crawled by all sorts of things. Most are innocuous indexing robots, but some are definitively not so nice, like these two requests:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2020/scanners.jpg" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2020/scanners.jpg" alt="nginx logs showing port scanning attacks."&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2020/scanners.jpg" alt="nginx logs showing port scanning attacks."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Most of those requests seem fine, but the two in red are probably some kind of attack.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;So the headline here is: keep all software updated, keep an eye on access logs and maybe go through nginx safety guides such as &lt;a href="https://www.cyberciti.biz/tips/linux-unix-bsd-nginx-webserver-security.html"&gt;this&lt;/a&gt; and &lt;a href="https://geekflare.com/nginx-webserver-security-hardening-guide/"&gt;this&lt;/a&gt;.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2019-11-06:/blog/2019/11/speeding-up-backend-graph-theory.html</id>
    <title type="html">Speeding Up the Backend with Graph Theory</title>
    <published>2019-11-06T22:29:15Z</published>
    <updated>2019-11-06T22:29:15Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2019/11/speeding-up-backend-graph-theory.html" type="text/html"/>
    <content type="html">
&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/2019/11/speeding-up-backend-graph-theory.html"&gt;
      &lt;img class="lazy" data-src="/blog/assets/images/2019/0076-final-results.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0076-final-results.png" alt="Alternative text to describe image."&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Here at Sensor Tower we handle large volumes of data, so to keep things snappy for our customers we need to think carefully about how we process and serve that data.&lt;/p&gt;

&lt;p&gt;Understanding the data we’re handling is a fundamental part of improving the way we serve it, and by analyzing how an important backend service worked, we were able to speed it up by a factor of four.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;&lt;em&gt;This post was originally posted in the &lt;a href="https://sensortower.com/blog/speeding-up-the-backend-with-graph-theory"&gt;Sensor Tower blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id="background"&gt;Background&lt;/h2&gt;

&lt;p&gt;We have many user-facing endpoints in Sensor Tower. So many, in fact, that we have numerous dashboards to keep tabs on how the system behaves.&lt;/p&gt;

&lt;p&gt;A few months ago, we noticed that a particular and very important endpoint was very sluggish: While all the other endpoints of the same type had &amp;lt;50ms latencies, this particular service took a leisurely 300 to 500ms to respond. Here’s a diagram of how that looked:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0070-sadface-backend.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0070-sadface-backend.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0070-sadface-backend.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;The customer doesn’t look very happy up there! So, we decided to take some time and do an in-depth analysis of that endpoint.&lt;/p&gt;

&lt;p&gt;Okay, now to a more serious diagram. Here’s what that endpoint looked like:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0070-diagram.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0070-diagram.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0070-diagram.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;The numbered steps in the diagram above perform the following operations:
1. Decode a Protobuf string and build a Ruby object from it;
2. Modify the object;
3. Encode the Ruby object back to Protobuf.&lt;/p&gt;

&lt;p&gt;In essence, the endpoint receives Protobuf-encoded strings, does some work on them, and returns a processed version of the Protobuf-encoded string to the client. If you don’t know what Protobuf is, that’s okay; I didn’t either. You can think of it as something similar to JSON: A serialized tree structure encoded using a binary format rather than text.&lt;sup id="fnref:1" role="doc-noteref"&gt;&lt;a href="#fn:1" class="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Once we pinpointed that the endpoint slowness was due to Protobuf parsing, the next step was to try and find bottlenecks in the algorithm. The proper way to do this (which is not to just to read the code thoroughly) is by using a profiler.&lt;/p&gt;

&lt;p&gt;With the help of &lt;a href="https://ruby-prof.github.io"&gt;ruby-prof&lt;/a&gt; (generate profile data) and &lt;a href="https://kcachegrind.github.io"&gt;KCacheGrind&lt;/a&gt; (view profile data), we were able to identify two methods, &lt;code&gt;#find_all&lt;/code&gt; and &lt;code&gt;#encode&lt;/code&gt;, that took a large portion of the CPU time:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0070-profiler.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0070-profiler.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0070-profiler.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;While a profiler is a useful tool to identify potential problems, it has its limitations. Profiling helps you visualize data from &lt;em&gt;a single run&lt;/em&gt; through your code. That might be fine if you’re analyzing a simple algorithm that deals with very homogeneous input, but is not really enough in the case of our back-end service, which receives thousands of very different inputs every hour.&lt;/p&gt;

&lt;p&gt;In other words, we also needed to validate the profiler results with more data.&lt;/p&gt;

&lt;p&gt;Taking this understanding into account, we opted for benchmarking a few thousand requests. Specifically, we benchmarked the &lt;code&gt;#find_all&lt;/code&gt; and &lt;code&gt;#encode&lt;/code&gt; methods and found out that, while the time they consumed relative to the total time varied, the sum of those two methods took almost 100 percent of the total time of the entire endpoint. At this point we knew we could focus our attention on these two methods.&lt;/p&gt;

&lt;p&gt;Naturally, the first step was to understand what each of those methods did.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#find_all&lt;/code&gt; is responsible for decoding and modifying the object (steps one and two in the diagram), while &lt;code&gt;#encode&lt;/code&gt; is exactly what the name implies: It re-encodes the modified Ruby object back to Protobuf (step three).&lt;/p&gt;

&lt;p&gt;With that said, let’s go through the optimizations performed in both of these methods.&lt;/p&gt;

&lt;h2 id="first-optimization-encode"&gt;First Optimization: Encode&lt;/h2&gt;

&lt;p&gt;Before we dive into the first optimization, let’s first explain how exactly the encoding/decoding processing works. Here’s an overview of what they do:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0071-decode-encode.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0071-decode-encode.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0071-decode-encode.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;*Those Strings aren't really Protobuf -- because it is a binary encoding, it isn't too easy on the eyes, so for the sake of readability we're using this pseudo-JSON representation.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;There are two things we need to point out about this decoding/encoding process that might not be obvious:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Both encoding and decoding work recursively, starting at the root and finding their way down to the leaves;&lt;/li&gt;
  &lt;li&gt;Each node in the Ruby object also contains the original Protobuf string for that node. So for instance the node &lt;code&gt;C&lt;/code&gt; in the example above also contains the following string: &lt;code&gt;"{ C: { B: [A]}, F: [D, X1] }"&lt;/code&gt;; the node &lt;code&gt;B&lt;/code&gt; contains this other string: &lt;code&gt;"{ B: [A] }"&lt;/code&gt;, and so on.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now we’re ready to understand the actual optimization.&lt;/p&gt;

&lt;p&gt;Let’s take a detailed look at the modification process:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0072-decode-encode-detail.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0072-decode-encode-detail.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0072-decode-encode-detail.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;One of the most fundamental steps of any optimization is avoiding repeated work. If we look closely, we can see that the nodes in blue (A, B, D) &lt;em&gt;were not modified&lt;/em&gt;: Look at the strings generated by decode (left side, in yellow) and compare them with the ones generated by encode (blue, to the right)—they’re identical! Conversely, nodes in red (C, F) were indeed modified: The strings are different. So, now we know there is some potentially repeated work going on.&lt;/p&gt;

&lt;p&gt;The first optimization leveraged this repeated work. Instead of always encoding every single node, we now encode &lt;em&gt;only those nodes that were modified&lt;/em&gt;. All the rest of the nodes already have a valid Protobuf string stored as an instance variable, and that string is identical to what we would obtain if we were to run &lt;code&gt;#encode&lt;/code&gt; on them.&lt;/p&gt;

&lt;p&gt;The actual code change to implement this was quite simple: Just a matter of adding a &lt;code&gt;dirty&lt;/code&gt; flag to each node, and marking the node as &lt;code&gt;@dirty = true&lt;/code&gt; if it or one of its descendants was modified.&lt;/p&gt;

&lt;p&gt;This optimization alone reduced the endpoint’s execution time by 30 percent. Here’s the execution time chart right after deploying the optimization:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0073-result-optim-1.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0073-result-optim-1.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0073-result-optim-1.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;h2 id="second-optimization-finding-a-node"&gt;Second Optimization: Finding a Node&lt;/h2&gt;

&lt;p&gt;The first optimization worked on the &lt;code&gt;#encode&lt;/code&gt; method, so the natural next step was to look at the other time-consuming method, &lt;code&gt;#find_all&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As we briefly mentioned, &lt;code&gt;#find_all&lt;/code&gt; is responsible for two things: Decoding the Protobuf string into a Ruby object and modifying the object itself.&lt;/p&gt;

&lt;p&gt;Unfortunately, there is no way of knowing beforehand if we’ll need to modify anything or not, so we’ll always have to do the decoding step. But what about the other thing &lt;code&gt;#find_all&lt;/code&gt; does, modifying the object?&lt;/p&gt;

&lt;p&gt;Before diving in, let’s recall a few things:
1. Protobuf is a tree-based data structure;
2. The trees we receive have no internal order to take advantage of;
3. Our algorithm searches for specific nodes and removes them from the tree;
4. We don’t know what the trees look like beforehand.&lt;/p&gt;

&lt;p&gt;Before this optimization, &lt;code&gt;#find_all&lt;/code&gt; was running a simple tree traversal to try and find those specific nodes mentioned in step three above. This is an acceptable approach when your input is small or when you’re not too worried about response time, but when you have massive inputs and want to deliver the smallest possible runtime, tree traversals can be a problem: They have linear time complexity (&lt;code&gt;O(n)&lt;/code&gt;, where n is the number of nodes).&lt;/p&gt;

&lt;p&gt;Once we know the path to a node, though, accessing it is very cheap: It can be done in logarithmic time, &lt;code&gt;O(log n)&lt;/code&gt;. This is possible because of a mathematical property of trees: Tree height is roughly a logarithmic function of the amount of nodes (it might degenerate into a linear function as well, but let’s leave those explanations to the textbooks), thus the average-case maximum path length to a node (that is, from the root down to the deepest leaf) is also bound to that same logarithmic constraint.&lt;/p&gt;

&lt;p&gt;So, we started looking closely to which paths we were going through to access those few nodes we wanted to remove. Ideally, there would be a single, universal path found in all the trees we ever encounter. That, way we could store that single path and always be guaranteed of finding the nodes we want to. Conversely, the worst possible outcome would be that every tree had a unique path to those nodes.&lt;/p&gt;

&lt;p&gt;The truth lied somewhere in between those two extremes (thankfully for us, it leaned more towards the former rather than the latter). Here’s a chart of the amount of different paths we found over time (the two curves represent paths to two of the nodes we want to remove):&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0070-paths-log.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0070-paths-log.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0070-paths-log.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Without going into too much detail about that chart, just notice that it is &lt;em&gt;very logarithmic!&lt;/em&gt; This is excellent for us, because it means that with a relatively small amount of paths we can find a very large percentage of the total nodes we want to find (and for the few we don’t, not finding them is okay). The next chart compares what we actually found (logarithmically growing paths) with the worst possible scenario mentioned previously:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0070-worst-case.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0070-worst-case.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0070-worst-case.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;So, the second optimization was, in the end, also very simple: We simply collected a large enough amount of different paths and then traversed to them in logarithmic time instead of doing a full tree traversal that takes linear time to find the nodes we wanted to modify. This was responsible for a 300 percent speedup:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0075-result-optim-2.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0075-result-optim-2.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0075-result-optim-2.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;h2 id="caveats"&gt;Caveats&lt;/h2&gt;

&lt;p&gt;You might have noticed that this method is not perfect in the sense that it doesn’t always find all the nodes that we would’ve found using a complete tree traversal. This is quite true: The optimization comes with an accuracy trade-off. While this might be a deal-breaker for systems where you actually need 100 percent accuracy at all times, this wasn’t really a problem for us; Missing a few nodes out of the several thousand we process each hour wasn’t really a big deal.&lt;/p&gt;

&lt;p&gt;As time passes, however, and different trees keep coming in, the precision of this approach eventually declines to a level that is significant even for our not-too-strict requirements. This happens slowly, because of how different paths appear (following a logarithmic function), but surely—our accuracy was ever-descending because we had used a fixed number of paths.&lt;/p&gt;

&lt;p&gt;After employing our optimized algorithm to the payload and responding to the request, we post-process a subset of the the requests in the background and dynamically update the path definitions. This way, we always have a very high success rate on the parsing but keep the latency of responding to a particular request low.&lt;/p&gt;

&lt;h2 id="final-results"&gt;Final Results&lt;/h2&gt;

&lt;p&gt;Here’s a chart showing our execution time before and after we rolled out both optimizations:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/2019/0076-final-results.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/2019/0076-final-results.png" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/2019/0076-final-results.png" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;We effectively reduced execution time from 300-500ms to 80ms with almost no impact to the user.&lt;/p&gt;

&lt;h2 id="notes"&gt;Notes&lt;/h2&gt;

&lt;div class="footnotes" role="doc-endnotes"&gt;
  &lt;ol&gt;
    &lt;li id="fn:1" role="doc-endnote"&gt;
      &lt;p&gt;Protobuf, or &lt;a href="https://en.wikipedia.org/wiki/Protocol_Buffers"&gt;Protocol buffers&lt;/a&gt;, is a binary serialization method developed by Google. &lt;a href="#fnref:1" class="reversefootnote" role="doc-backlink"&gt;↩&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2019-08-30:/blog/2019/08/creating-more.html</id>
    <title type="html">My attempt at creating more</title>
    <published>2019-08-30T00:00:00Z</published>
    <updated>2019-08-30T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2019/08/creating-more.html" type="text/html"/>
    <content type="html">&lt;p&gt;I began blogging in the now prehistoric late 2000s.&lt;/p&gt;

&lt;p&gt;I’ve done a few blogs about different subjects (computer science, algorithms, web development, short stories and political ramblings). I’ve had blogs on Blogspot, Wordpress and, more recently, Medium.&lt;/p&gt;

&lt;p&gt;Those platforms were (or are, I suppose) an easy way to spew your ideas over the Internet while also being nice and comfy for other people to actually read (this last point is important for the CSS-challenged such as yours truly). In other words, those services Got Shit Done™.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;Alas, as I opened my eyes to the wonders of web development I started noticing a few things. First, Wordpress is written in PHP, which is gross (just kidding). Second, you don’t really control much: you can pick themes or whatever, but you won’t have the full control you’d have by creating a website from scratch or nearly scratch. Third, and maybe a corollary to the previous point, that stuff is &lt;em&gt;bloated&lt;/em&gt;. There’s approximately 3 terabytes of mostly useless JavaScript, ads and all kind of crap I don’t care about.&lt;/p&gt;

&lt;p&gt;But most importantly, I understood the hidden costs of most “free” web services. You don’t really own anything. You provide content, and Wordpress or Google or whoever package that content into a neat bundle and servce it to your audience together with whatever else (&lt;em&gt;cough, trackers&lt;/em&gt;) they see fit.&lt;/p&gt;

&lt;p&gt;That’s one of the reasons that pushed me towards a less-walled-garden approach towards blogging. But there’s a nother reason as well.&lt;/p&gt;

&lt;p&gt;As &lt;a href="https://code.divshot.com/geo-bootstrap/"&gt;many others&lt;/a&gt;, I have fond memories of the late-90s/early-2000s Web 1.0 Internet. There is something warm and fuzzy about those beautifully terrible Geocities pages. They pierced the eyes of the viewer but were wondrous in a way. As I said, I’m not alone: Web 1.0 nostalgia is definitively &lt;a href="https://gizmodo.com/the-great-web-1-0-revival-1651487835"&gt;on the rise&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But why? What is not to like in our world of beautiful walled gardens? Surely it is better than those gross-looking Web 1.0 fan sites about some crappy GameBoy game, right? …Right?&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/fan_page_screenshot.png" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/fan_page_screenshot.png" alt="View of an old website about Pokemon"&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/fan_page_screenshot.png" alt="View of an old website about Pokemon"&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;We're soooo cooler than this in 2019.&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Wrong.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Well, in many ways the Internet has of course improved over time. It has useful things like search engines and Wikipedia, and convenient subscription-based entertainment like Netflix. It has a whole bunch of nice stuff I could spend hours blabbering about.&lt;/p&gt;

&lt;p&gt;But it also has a lot of problems. At this point there are surely many PhD theses about most of them, so I won’t bother. I’m just going to recommend one &lt;a href="https://www.nytimes.com/2019/08/11/world/americas/youtube-brazil.html"&gt;New York Times article&lt;/a&gt; that explains how YouTube indirectly helped elect a buffoon that &lt;a href="https://extra.globo.com/noticias/brasil/bolsonaro-faz-piada-com-oriental-tudo-pequenininho-ai-veja-video-rv1-1-23668287.html"&gt;makes high school-tier penis jokes&lt;/a&gt; as president of Brazil.&lt;/p&gt;

&lt;p&gt;Now, the specific problem with today’s Internet that I feel is most relevant regarding blogging is how we’re gravitating towards all these “free” services all the time. Medium, for instance, is so &lt;em&gt;nice looking&lt;/em&gt; that one doesn’t even think of perhaps using something else. But what happens when &lt;em&gt;everyone&lt;/em&gt; uses Medium? First: all the blogs look exactly the same, which is lame. Second, Medium gets all that content and traffic for itself, for free.&lt;/p&gt;

&lt;p&gt;Of course, not everyone is skilled enough to build a personal blog from scratch. I am just barely able, as you can see from my lackluster front-end skills (I promise you I’m good on back-end things). So I’m definitively not dismissing the inclusiveness that services like Medium offer.&lt;/p&gt;

&lt;p&gt;But as I searched for a way out of the walled gardens and fiddled with &lt;a href="https://jekyllrb.com/"&gt;Jekyll&lt;/a&gt; for a while, I figured I might as well just &lt;a href="https://tjcx.me/posts/consumption-distraction/"&gt;build something to call my own&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="so-i-built-this"&gt;So I built this.&lt;/h2&gt;

&lt;p&gt;The goals were to create the simplest possible blogging system with as little fluff as possible. It should meet what I defined as basic blogging needs: list posts, show post, use tags, use images etc. And also not have 3 terabytes of JavaScript split in 90 requests just to show a fancy menu button.&lt;/p&gt;

&lt;p&gt;So I started messing with Nanoc, an excellent Ruby library for static page generation, and came up with &lt;a href="https://github.com/lbrito1/sane-blog-builder"&gt;this bad boy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I won’t pretend these ideas are new. They aren’t! I feel there’s been an increasing amount of &lt;a href="https://code.divshot.com/geo-bootstrap/"&gt;Web 1.0 nostalgia&lt;/a&gt; going on, and a big part of that is probably fueled by &lt;a href="https://tjcx.me/posts/consumption-distraction/"&gt;similar sentiments&lt;/a&gt; as those I described. The &lt;a href="https://thebestmotherfucking.website/"&gt;longing for simplicity&lt;/a&gt; in a world of trillions of new JS frameworks is also quite widespread these days.&lt;/p&gt;

&lt;p&gt;This small project is nothing special. There are &lt;a href="https://github.com/remko/blog-skeleton"&gt;much better projects&lt;/a&gt; &lt;a href="https://clarkdave.net/2012/02/building-a-static-blog-with-nanoc/"&gt;available for free&lt;/a&gt; on the Internet done by people that actually know what they’re doing with a CSS file. This here is just a tiny vase with some ugly flowers – it would be ridiculous to compare it to the beautiful walled gardens of Medium or Wordpress. But, ugly as they are, they’re &lt;strong&gt;mine&lt;/strong&gt;!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2018-09-03:/blog/2019/01/halving-page-sizes-with-srcset.html</id>
    <title type="html">Halving page sizes with srcset</title>
    <published>2018-09-03T00:00:00Z</published>
    <updated>2018-09-03T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2019/01/halving-page-sizes-with-srcset.html" type="text/html"/>
    <content type="html">&lt;p&gt;&lt;a href="https://www.webbloatscore.com/"&gt;Web bloat&lt;/a&gt; is &lt;a href="http://idlewords.com/talks/website_obesity.htm"&gt;discussed&lt;/a&gt; a lot nowadays. Web pages with fairly straightforward content — such as a Google search results page — are substantially bigger today than they were a few decades ago, even though the content itself hasn’t changed that much. We, web developers, are at least partly to blame: laziness or just &lt;a href="http://www.haneycodes.net/npm-left-pad-have-we-forgotten-how-to-program/"&gt;bad programming&lt;/a&gt; are definitively part of the problem (of course, laziness might stem from a tight or impossible deadline, and bad code might come from inexperienced programmers — no judgment going on here).&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;But here at Guava we believe that software should not be unnecessarily bloated, even though it could be slightly easier to develop and ship. We believe in delivering high quality production code, and a part of that is not taking the easy way out in detriment of page size.&lt;/p&gt;

&lt;p&gt;We frequently have to start working on long-running software that has more than a few coding shortcuts that were probably necessary at the time to ship something quickly to production, but are now aching for optimization. Sometimes the improvements are too time-consuming to be worth our trouble, but sometimes they are an extremely easy win.&lt;/p&gt;

&lt;p&gt;Such is the case of separating image assets by pixel density (DPI). As the name implies, DPI (dots per inch) is the amount of dots (or pixels, in our case) that fit in a square inch of screen real estate. The exact definition varies according to context, so for the sake of readability we’ll say that low DPI means the average desktop or laptop screen and budget smartphones, while high DPI means the average smartphone, tablet or higher-resolution computer screens (e.g. Retina displays and 4k monitors).&lt;/p&gt;

&lt;p&gt;Nowadays, smartphone customers are important to most online retail businesses, which means that we should serve high DPI images &lt;em&gt;when necessary&lt;/em&gt;. The “when necessary” part is important because the easy way out is to &lt;em&gt;always&lt;/em&gt; serve high DPI assets, even though the client device might not need them. The problem with this is that high DPI images are roughly 4 times as big as their low DPI counterparts, so low DPI devices would be getting unnecessarily big images for nothing at all — web bloat!&lt;/p&gt;

&lt;p&gt;Serving different assets according to the client’s DPI was not a trivial task a few years ago, which means that the web is probably filled with pages that still serve high DPI assets by default to all client browsers. But now that HTML5 is widely adopted we can make good use of &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/img"&gt;srcset&lt;/a&gt; to do just that. To each their own: &lt;code&gt;srcset&lt;/code&gt; takes a list of different images and serves the most appropriate one to each client. In image-heavy sites such as retail stores this is an excellent tool to optimize average page size and save a good deal of bandwidth — which means saving money. Smaller images also take less time to load, so customers will also see product images faster than before.&lt;/p&gt;

&lt;p&gt;This very simple change allowed us to decrease page sizes in one of our projects over 50% in some of its most-accessed endpoints, and an overall average 25% page size reduction for low DPI customers. Considering that some of the pages were 4 or 5MB big, halving those sizes was a great improvement to our customers — even more so considering that some of them might access our site on low-quality mobile networks, which can be excruciatingly slow sometimes. Considering the proportion of low DPI customers we have on an average day, this improvement saved our client some 7.5% of bandwidth.&lt;/p&gt;

&lt;p&gt;Now that we’ve got some hindsight, it seems glaringly obvious that we should have been using this feature all along. But more often than not, extremely simple optimizations such as the one we described are overlooked by less experienced teams or worse — deemed “not important” by management because customers nowadays supposedly can spare a few megabytes per page (that may be so, but they don’t want to!).&lt;/p&gt;

&lt;p&gt;We think that bloated web pages hurt everyone involved: web developers, customers and businesses. We strive to achieve what we think is good quality web code: that which delivers optimized, slim web pages to all clients.&lt;/p&gt;

&lt;p&gt;By &lt;a href="https://medium.com/@lbrito"&gt;Leonardo Brito&lt;/a&gt; on &lt;a href="https://medium.com/p/f82a1c5deb26"&gt;January 14, 2019&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/@lbrito/halving-page-sizes-with-srcset-f82a1c5deb26"&gt;Canonical link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Exported from &lt;a href="https://medium.com"&gt;Medium&lt;/a&gt; on May 1, 2019.&lt;/p&gt;
</content>
    <summary type="html">There are many different devices accessing the internet, and they all have different screens. By using srcset to optimize the images served by our webapp, we reduced page sizes by up to 50%.</summary>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2018-09-03:/blog/2018/09/10-ways-not-to-do-a-big-deploy.html</id>
    <title type="html">10 ways not to do a big deploy</title>
    <published>2018-09-03T00:00:00Z</published>
    <updated>2018-09-03T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2018/09/10-ways-not-to-do-a-big-deploy.html" type="text/html"/>
    <content type="html">&lt;p&gt;Ideally, deploys should be small, concise, easily revertible, fast and with a small or nil footprint on the database. However, no matter how awesome you are, sometimes that is just unattainable and you end up needing to deploy something that is just the opposite: big, messy, hard to revert, painfully slow and rubbing the DB the wrong way. If the deploy messes with a mission-critical part of your software, all the worse for you.&lt;/p&gt;

&lt;p&gt;But there are actually many ways you can make those situations even worse. Here are a few bullet points you can follow to guarantee a nightmarish deploy complete with nasty side-effects that will haunt you and your coworkers for days to come.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id="dont-make-aplan"&gt;1. Don’t make a plan&lt;/h2&gt;

&lt;p&gt;Plans suck. They take time and effort, and don’t add any new features to your software. Planning a deploy requires thinking carefully about what it should do and, more importantly, what it shouldn’t do (but potentially could). A good deploy plan is a step-by-step happy path that is written clearly and concisely, followed by a list of everything nasty that can happen. Making a deploy plan is basically trying to cover as many blind spots as you can before pulling the trigger. But, of course, you and your team are code ninjas or master software crafters or whatever the hippest term is nowadays, and you don’t need a plan! Just wing it. Press the button and solve every problem that might arise in an ad-hoc fashion. What could go wrong?&lt;/p&gt;

&lt;h2 id="dont-scheduledowntime"&gt;2. Don’t schedule downtime&lt;/h2&gt;

&lt;p&gt;Downtime sucks: it usually is in odd hours, late in the night or early in the morning, when customers are fast asleep (and you would very much like to be as well). Why bother blocking public access and redirecting customers to a nice “scheduled maintenance page”? Why gift you and your team with peace of mind and a clear timeframe to work with if you can feel the rush of breaking stuff in production with live customers? Production debugging is the best kind of debugging! Confuse your customers with inconsistent states and leave them waiting while your team tries to fix those bugs that were definitively fixed last Friday night.&lt;/p&gt;

&lt;h2 id="dont-have-a-great-logsystem"&gt;3. Don’t have a great log system&lt;/h2&gt;

&lt;p&gt;Logs are for buggy software, you won’t need them. Why spend time and possibly money with a great logging-as-a-service (LaaS) platform? Just have your whole team &lt;code&gt;ssh&lt;/code&gt; into production and watch the log tails. Or, even better, use a terrible LaaS that is slow, unreliable and has a confusing user interface so everyone can get frustrated trying to find errors during the deploy.&lt;/p&gt;

&lt;h2 id="dont-have-a-bugtracker"&gt;4. Don’t have a bug tracker&lt;/h2&gt;

&lt;p&gt;See above: just like logs, bug trackers are also lame. Your awesome PR won’t have any bugs, now, will it? Regressions never happen under your watch. Also, who needs to track exceptions with a great, fast, reliable bug tracking platform when you have logs available? Aren’t you hacker enough to &lt;code&gt;grep&lt;/code&gt; every single exception that might be raised?&lt;/p&gt;

&lt;h2 id="dont-have-a-stagingserver"&gt;5. Don’t have a staging server&lt;/h2&gt;

&lt;p&gt;Staging servers are a waste of resources, both time and money. What is the point of having a close-to-exact copy of your production servers, which by this point are radically different from your development environment? Sure, containerization already &lt;em&gt;kind of&lt;/em&gt; abstracts many of those differences, but (hopefully) you have network settings, 3rd-party APIs and other stuff that aren’t the same in development, even with containers. So be bold and make the leap from development right to production!&lt;/p&gt;

&lt;h2 id="dont-check-your-envvars"&gt;6. Don’t check your env vars&lt;/h2&gt;

&lt;p&gt;Your project only has like 80 different access tokens, API keys, DB credentials and cache store credentials spread over half a dozen YAMLs. Super easy to keep track of and super hard to mess up with your production, development and (hopefully) staging environments. Don’t triple-check the variables that might have been changed in the deploy, and you’ll secure a few hours of painful debugging in the near future.&lt;/p&gt;

&lt;h2 id="dont-guarantee-data-consistency-post-deploy"&gt;7. Don’t guarantee data consistency post-deploy&lt;/h2&gt;

&lt;p&gt;In a previous step you were told already to make sure that customers can keep using your software mid-deploy, so we’re halfway there already to guaranteeing poor data consistency. Make sure you haven’t mapped out all the points your new code might touch the DB, particularly the DB structure itself. If anything goes wrong, just revert the commit and rollback — don’t ever worry about becoming orphaned or inconsistent.&lt;/p&gt;

&lt;h2 id="dont-prepare-for-a-laterollback"&gt;8. Don’t prepare for a late rollback&lt;/h2&gt;

&lt;p&gt;If everything else fails… wait, it won’t! Some problems can surface during the deploy, sure, but we won’t need to rollback &lt;em&gt;after&lt;/em&gt; it is done, right? Right? After everything is settled, and you made a plan (which you totally shouldn’t, remember?) and followed it step-by-step, and all went well, you shouldn’t need to rollback. But let’s say it happens, and a few hours (or days) after the deploy you need to go back to the previous commit/tag/whatever you use. New data will have flowed which might need to be manually converted back to something manageable by the previous version of your software. Don’t think about it, don’t plan for it — it isn’t likely to happen. And if it does, you will have a heck of a time working on oddball and edge cases late in the night. What is not to love?&lt;/p&gt;

&lt;h2 id="dont-communicate-efficiently-with-yourteam"&gt;9. Don’t communicate efficiently with your team&lt;/h2&gt;

&lt;p&gt;You already know you should have terrible log and error tracking systems. Add insult to injury and don’t talk to your coworkers in a quick, direct and clear way. Long pauses are great for dramatic effect, especially when your coworkers are waiting for a timely answer. Be vague about what you’re doing. Hit the rollback button and “forget” to tell people about it. In general, just be as confusing and unavailable as possible.&lt;/p&gt;

&lt;p&gt;Following all of the points above might lead to a “perfect storm” situation, and making sure you don’t follow them will surely make things easier on you and your team. But even if you have great deploy practices in place, sometimes things just fall apart. There will always be blind spots, and it is in their nature to be more or less unpredictable. That is just the way things are with software development. Which leads us to our 10th and final point in this guide to terrible deploys:&lt;/p&gt;

&lt;h2 id="dont-be-patient-and-understanding-with-your-coworkers-if-everything-fallsapart"&gt;10. Don’t be patient and understanding with your coworkers if everything falls apart!&lt;/h2&gt;

&lt;p&gt;By &lt;a href="https://medium.com/@lbrito"&gt;Leonardo Brito&lt;/a&gt; on &lt;a href="https://medium.com/p/f536d1ad9a5a"&gt;September 3, 2018&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/@lbrito/10-ways-not-to-do-a-big-deploy-f536d1ad9a5a"&gt;Canonical link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Exported from &lt;a href="https://medium.com"&gt;Medium&lt;/a&gt; on May 1, 2019.&lt;/p&gt;
</content>
    <summary type="html">Ideally, deploys should be small, concise and easily revertible. However, sometimes everything just goes kaput. Let's take a look on just how miserable a deploy can get.</summary>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2018-06-12:/blog/2018/06/working-remotely-in-a-non-remote-company.html</id>
    <title type="html">Working remotely in a non-remote company</title>
    <published>2018-06-12T00:00:00Z</published>
    <updated>2018-06-12T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2018/06/working-remotely-in-a-non-remote-company.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/goiabada/1*mgVZOuAHmp9Ipm2asL0IQQ.jpg" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/goiabada/1*mgVZOuAHmp9Ipm2asL0IQQ.jpg" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/goiabada/1*mgVZOuAHmp9Ipm2asL0IQQ.jpg" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;We’re a small team here at Guava, and we’ve always considered ourselves &lt;em&gt;remote friendly.&lt;/em&gt; Most of us work remotely every now and then pushed by varied &lt;em&gt;force majeure&lt;/em&gt; situations— be it the flu, the need to supervise renovation or construction work at home, flash floods near the office, receiving guests at home or any number of other situations. We’ve also had a few of us working remotely for a few days or weeks while traveling to or back from a conference, or when visiting relatives that live out of town. In other words, remote working has always been a very temporary and circumstantial thing among us.&lt;/p&gt;

&lt;p&gt;We have a nice office (with hammocks!), excellent work equipment, great desk space, comfortable chairs, plenty of snacks and comfort food and an infinite supply of coffee. We’re also easygoing and overall pleasant people (well, most of us are) to work with several hours a day, and some of us are even mildly funny.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;I bid adieu to my coworkers, the coffee machine, the nice desk and the hammocks and traveled abroad to try out being a remote worker (some prefer the term &lt;em&gt;digital nomad&lt;/em&gt; — to me, it seems a bit preposterous to compare month-long stays in modern urban dwellings with electricity and wireless internet with the traditional nomadic lifestyle) for half a year. A few weeks before leaving, I read the interesting &lt;a href="https://basecamp.com/books/remote"&gt;Remote: Office Not Required&lt;/a&gt;, which I vividly recommend to anyone considering working remotely. Some of the challenges I faced during my time as a remote worker were foretold by the book, while others were a complete surprise. Here are a few of the things I learned firsthand about remote work:&lt;/p&gt;

&lt;h2 id="it-takes-time-toadjust"&gt;It takes time to adjust.&lt;/h2&gt;

&lt;p&gt;Your mind takes some time to adjust to working remotely. In many ways, working remotely feels like a completely new job — even if you’ve been in the same company and position for years.&lt;/p&gt;

&lt;p&gt;Some people have more trouble with this than others, but everyone will take some time to adjust. The important lesson here — for worker &lt;em&gt;and employer&lt;/em&gt; — is to have patience. Steep as it might be, the learning curve of adapting to remoteness will eventually plateau out.&lt;/p&gt;

&lt;h2 id="it-is-easier-when-youre-well-acquainted-with-yourteam"&gt;It is easier when you’re well acquainted with your team.&lt;/h2&gt;

&lt;p&gt;Starting a new project — be it a new job or just a new assignment involving different team members — may be daunting. Starting remote work already with good rapport with your coworkers helps tremendously, as you will feel more at ease to talk to people.&lt;/p&gt;

&lt;p&gt;It is important to feel comfortable enough to let your team know if something is going wrong right away, for example, as opposed to keeping it to yourself and suffer silently. Good rapport between developers also means it will be easier to understand each other when discussing technical problems.&lt;/p&gt;

&lt;h2 id="it-is-easier-when-youre-not-the-only-remoteperson"&gt;&lt;strong&gt;It is easier when you’re not the only remote person.&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Being “the remote guy” is a thing. People tend to forget people they don’t see every day, and you have to be comfortable with the low profile that comes with working remotely.&lt;/p&gt;

&lt;p&gt;Having other remote workers in your team helps a bit, creating that nice “we’re all on the same boat” feeling.&lt;/p&gt;

&lt;h2 id="you-need-to-be-able-to-communicate-verywell"&gt;You need to be able to communicate very well.&lt;/h2&gt;

&lt;p&gt;A huge part of working as a developer is being able to communicate well with other developers and with normal human beings. A programming genius that isn’t able to explain what he’s doing to his non-genius co-workers will likely not be a very good developer overall.&lt;/p&gt;

&lt;p&gt;Language is one of man’s great achievements as a species, and it carries the weight and complexity of thousands of years of mutation. Expressing yourself verbally is hard enough, but we also have a myriad of non-verbal communication cues that we unconsciously rely on to communicate with one another — which you won’t have as a remote worker (at least most of the times).&lt;/p&gt;

&lt;p&gt;Sure, you can occasionally call the HQ and video-chat with someone. But that is just not practical enough most of the times. As a remote worker, I find myself heavily relying on written communication with the rest of the team.&lt;/p&gt;

&lt;p&gt;Every challenge brings a chance to learn something. Because of the challenges and limitations of working remotely, the experience helps you grow professionally in some ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Guidance from more experienced coworkers or bosses is much more rarefied, which force you to exercise self-teaching and pro-activity.&lt;/li&gt;
  &lt;li&gt;The need to communicate more often through asynchronous text-first chats helps you develop language skills (and patience).&lt;/li&gt;
  &lt;li&gt;Working away from the office and your coworkers makes you appreciate them more when eventually returning to the HQ.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, there are many other benefits that come to mind when thinking about remote work, such &lt;a href="https://www.thriveglobal.com/stories/30386-a-2-year-stanford-study-shows-the-astonishing-productivity-boost-of-working-from-home"&gt;as increased productivity and financial savings&lt;/a&gt;, and there are already studies and books that got those covered. Which is not to say that remote work is some kind of panacea: it has fundamental disadvantages when compared with traditional work inside a brick-and-mortar office building, the most obvious and important being the lack of human contact with your fellow workers. The solitude and heavy reliance on written, asynchronous communication that often comes with remote work might not be your cup of tea.&lt;/p&gt;

&lt;p&gt;Remote work is neither a universal solution nor something completely out of reach for the average developer. Although it won’t be to everyone’s taste, it is definitively available to everyone (or should be). This semester abroad taught me that it is plainly possible and viable for a developer in a small, non-remote (but remote-friendly) software company to work far away from the HQ, and both sides have a lot to gain from the experience. It is really a win-win scenario, and people should try it more often.&lt;/p&gt;

&lt;p&gt;By &lt;a href="https://medium.com/@lbrito"&gt;Leonardo Brito&lt;/a&gt; on &lt;a href="https://medium.com/p/ce9e39645f85"&gt;June 12, 2018&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/@lbrito/working-remotely-in-a-non-remote-company-ce9e39645f85"&gt;Canonical link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Exported from &lt;a href="https://medium.com"&gt;Medium&lt;/a&gt; on May 1, 2019.&lt;/p&gt;
</content>
    <summary type="html">Here's what I learned after working half a year remotely in a remote-friendly (but mostly non-remote) company.</summary>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2018-03-05:/blog/2018/03/the-5-stages-of-dealing-with-legacy-code.html</id>
    <title type="html">The 5 stages of dealing with legacy code</title>
    <published>2018-03-05T00:00:00Z</published>
    <updated>2018-03-05T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2018/03/the-5-stages-of-dealing-with-legacy-code.html" type="text/html"/>
    <content type="html">&lt;p&gt;Yes, this article will use the &lt;a href="https://en.wikipedia.org/wiki/K%C3%BCbler-Ross_model"&gt;5 stages of grief&lt;/a&gt; as an analogy for something software development-related. There are at least a few thousand other articles with a similar motif (424,000 results for “grief stages software” according to &lt;a href="https://www.google.es/search?q=grief+stages+software&amp;amp;oq=grief+stages+software"&gt;Google&lt;/a&gt;). But bear with me for the next 5 minutes and I promise you’ll get something out of this — if nothing else, at least the smirk of those who read their past follies put on text by someone else.&lt;/p&gt;

&lt;p&gt;I have been working on a rather big Rails project for the past year and half. The project is nearly 7 years old, and has an all-too-common successful-startup-bought-by-industry-giant background story. In a project with this kind of background, some things are bound to happen: many developers of many skill ranges have come and gone, many software fads (cough, Meteor, cough), and above all else &lt;em&gt;a lot&lt;/em&gt; of legacy code that is, well, let’s put it nicely, &lt;em&gt;not so great&lt;/em&gt;. None of this should be taken personally in any way — it is just natural for such things to occur in such projects.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;These kinds of projects might be a bit unwelcoming and overwhelming to newcomers. In particular, the aforementioned &lt;em&gt;not so great&lt;/em&gt; legacy code might become a huge source of distress for the developer. Some degree of unpleasantness is probably unavoidable in any real-world software project, but choosing to deal with &lt;em&gt;not so great&lt;/em&gt; code in a productive, edifying way definitively helps to alleviate the potential nastinesses of working on legacy projects.&lt;/p&gt;

&lt;p&gt;Let us begin our analogy. The backstory is that you, dear reader, are assigned to work on a monolithic web app which is company X’s flagship product. It has been live for 5+ years and has thousands of customers. Three dozen developers have helped build the app over the years. The monolith uses a certain web framework since one of its earliest versions, and the team has haphazardly updated it to the latest stable version every once in a while. Leftovers are everywhere, and you have no clue if something is mission-critical code or just the remains of a failed, forgotten or otherwise removed feature or experiment. You are dumbstruck at the gigantic code-spaghetti and say to yourself:&lt;/p&gt;

&lt;h2 id="nope-this-cant-be-right-im-just-going-to-ignore-it-and-itll-goaway"&gt;Nope, this can’t be right. I’m just going to ignore it and it’ll go away.&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;This code is just so terrible, confusing or complex. I just can’t fathom any of it. It just can’t be right. It surely wasn’t meant for a human being to understand. I’m going to ignore it for a while so it can go away and get replaced by something nice and shiny.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You’re in &lt;strong&gt;Denial&lt;/strong&gt;. Take all the time you want — by the end of the day, the same code will still remain committed. Instead of ignoring it, take a deep breath and spend a couple of hours reading and trying to understand it. This is easier said than done for many reasons: the domain of your software might be complex, and thus the code might be as well; the code might be excessively complex or over-engineered; your employer might not be so keen of waiting for you to grasp the intricacies of some small code section; you might feel bad for not delivering the same performance you had on the last project/feature, when you were right up there in the productivity plateau and so on. Alas, the problem remains: at some point you will have to grasp the meaning of that code, even though it is pretty terrible.&lt;/p&gt;

&lt;p&gt;Time, patience and an understanding employer are the only things you need to get through. It also helps to have a senior coworker in this project that can guide you through the process. After some time you will begin to understand the codebase. You might start thinking that:&lt;/p&gt;

&lt;h2 id="i-hatethis"&gt;I hate this.&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ugh, &lt;em&gt;now&lt;/em&gt; I get it. Now I can see how the code is badly written, so underoptimized, naive or otherwise &lt;strong&gt;plain wrong&lt;/strong&gt;. What on Earth were they &lt;em&gt;thinking&lt;/em&gt;? This statement is so non-idiomatic it seems like another programming language; this query is so unnecessarily slow that I could go for a walk while the DB toils away; this Javascript is so ugly it could be used as a your-mom joke; …&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Anger&lt;/strong&gt; is what follows when you finally get just how bad the code really is. I don’t know if everything happens for a reason, but I believe every line of code exists for a reason — bad code included. &lt;a href="https://softwareengineering.stackexchange.com/questions/7530/how-do-you-deal-with-intentionally-bad-code"&gt;Very rarely&lt;/a&gt; it is the case that a developer knowingly chose to make a bad decision. More often:&lt;/p&gt;

&lt;p&gt;The developer might have been inexperienced, tired or overworked. The idiom of your programming language might have changed. The specifications might have changed mid-development. The libraries that are mature and well-known today might have not even existed. So much could have happened that explains why the code is the way it is.&lt;/p&gt;

&lt;p&gt;It is fine to try to understand the reason something is written the way it is, but try to resist the urge of blaming someone out of anger over the wretchedness of their code (you might find yourself in their shoes some day!). Once you decide not to &lt;code&gt;git blame&lt;/code&gt;all the time, you might start avoiding the elephant in the room:&lt;/p&gt;

&lt;h2 id="im-just-going-to-write-this-new-feature-and-everything-will-befine"&gt;I’m just going to write this new feature and everything will be fine.&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;I know the old code exists, but I’m just going to pretend it doesn’t for a while. I don’t want to deal with this right now. Besides, I don’t even know where to start, or whether I should start at all… is refactoring worth the trouble? Is it even possible?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you have the luxury of picking your own assignments, then it might be tempting to do some &lt;strong&gt;bargaining&lt;/strong&gt; and just ignore the bad legacy code for a while, promising to refactor some of it later— which is totally fine. At least for some time. But the problem never solves itself, and delaying improvements might have some nasty long term consequences. Once you start working around the limitations of bad code instead of addressing its problems, you are actually contributing to its evil legacy by creating more unwanted dependencies, more nonsensical interfaces, more unnecessarily complex methods, etc.&lt;/p&gt;

&lt;p&gt;But you can’t avoid the smelly code forever. One of these days, your boss will assign you to a certain issue…&lt;/p&gt;

&lt;h2 id="ive-been-assigned-to-solve-this-issue-which-relates-to-some-nasty-old-code-and-im-miserable"&gt;I’ve been assigned to solve this issue which relates to some nasty old code and I’m miserable.&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;I can’t believe I’ve been assigned to improve this legacy stuff — I’ve successfully avoided it for months now! This just &lt;em&gt;sucks&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You knew what was coming towards your plate for some time, but now that it’s finally here, you’re suddenly &lt;strong&gt;depressed&lt;/strong&gt;. That’s fine too, but don’t fall into self-pity: having to deal with bad code might greatly benefit your knowledge as a software developer. Think of it as &lt;em&gt;having room for improvement.&lt;/em&gt; Which nicely leads us to…&lt;/p&gt;

&lt;h2 id="hmmm-maybe-i-can-improvethings"&gt;Hmmm, maybe I can improve things…&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Wait a minute, I think I know how to improve this…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the a-ha! moment. You &lt;strong&gt;accept&lt;/strong&gt; the reality of legacy code, and instead of mulling over how bad it is, you realize that those columns — or tables — are useless, that those dependencies are no longer used, that there are many methods that are now superfluous, and so on. The tipping point here is when you gain enough knowledge of the codebase to start systematically questioning why the code is the way it is — and not just assuming that it is right, or that all of it is necessary.&lt;/p&gt;

&lt;p&gt;This last stage of legacy code grief takes, of course, the most time to arrive. In my case it started arriving many months into the project, when I found out a few ways to optimize our spec suite which &lt;a href="https://goiabada.blog/improving-spec-speed-in-a-huge-old-rails-app-8f3ab05a33f9"&gt;resulted in a 41% speedup in CI execution&lt;/a&gt;. As time passes and I understand the project more deeply, new ideas of how to optimize or otherwise improve some part of it start popping up more naturally. Eventually you reach a point in which you can see bad code more as an opportunity to improve something (and maybe learn something new in the process) or even try out different solutions or technologies rather than a huge grief you have to deal with as a part of your day job.&lt;/p&gt;

&lt;h2 id="its-all-part-of-theprocess"&gt;It’s all part of the process&lt;/h2&gt;

&lt;p&gt;Working on a legacy project might not be as exciting as building a shiny new software from scratch, but it has its own merits. Unlike working on something new, on a legacy project you will need to understand how &lt;em&gt;other people&lt;/em&gt; solved some problem — which includes understanding all the limitations they might have had at that time. It is also a humbling experience: the bright glow of hindsight exposes all the scars of bad programming and bad decisions made in the past, and if you’re being honest with yourself you’ll have no other option than to admit that &lt;em&gt;it could have been you&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;By &lt;a href="https://medium.com/@lbrito"&gt;Leonardo Brito&lt;/a&gt; on &lt;a href="https://medium.com/p/6d578205beeb"&gt;March 5, 2018&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/@lbrito/the-5-stages-of-dealing-with-legacy-code-6d578205beeb"&gt;Canonical link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Exported from &lt;a href="https://medium.com"&gt;Medium&lt;/a&gt; on May 1, 2019.&lt;/p&gt;
</content>
    <summary type="html">Working on a legacy project sometimes feels like a journey through grief. It kind of is, but not really: there's always something important we can learn in the process.</summary>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2017-09-04:/blog/2017/09/improving-spec-speed-in-a-huge-old-Rails-app.html</id>
    <title type="html">Improving spec speed in a huge, old Rails app</title>
    <published>2017-09-04T00:00:00Z</published>
    <updated>2017-09-04T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2017/09/improving-spec-speed-in-a-huge-old-Rails-app.html" type="text/html"/>
    <content type="html">&lt;p&gt;We got a 6-year-old Rails app with ~370k LOC and a ~6k-test suite which took 24 minutes to complete. Not good! We took a few days off of the main project to see if we could make things better.&lt;/p&gt;

&lt;p&gt;More often than not, test suites are the nasty underbelly of a Rails app. Size and age just aggravate the problem. Tests are seldom a high priority in any project, and speed might not be an issue at all in smaller apps where the whole test suite might take just a few seconds to complete. As the project grows and the CI takes increasingly longer to complete, spec speed suddenly becomes more of an issue.&lt;/p&gt;

&lt;p&gt;“Small” and “new” are not exactly the case for a certain Rails project we’re working on here at Guava. We’re talking about a 6-year-old e-commerce portal with ~370k LOC, a couple million customers and a ~6k-test, 300-spec suite which took, on average, a whopping 24 minutes to complete in our CI. &lt;em&gt;Not good!&lt;/em&gt; So we took a couple of days off the main project to see if we could make things better — or less worse.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id="preliminariessomeone-must-have-solvedthis"&gt;Preliminaries — someone &lt;em&gt;must&lt;/em&gt; have solved this!&lt;/h2&gt;

&lt;p&gt;The first step we took was, of course, to do some “literature review”, so to speak. Googling “improving rspec spec speed” will yield dozens of “10 ways to improve spec speed”-like articles, with more or less the same tips repeated over and over: make less DB hits, make sure the tests don’t do external requests, don’t use &lt;code&gt;js: true&lt;/code&gt; unless you need to, etc. Solid points, but all of them were either already used in our codebase or impractical to implement in a project this size.&lt;/p&gt;

&lt;p&gt;We would need to find our own solutions.&lt;/p&gt;

&lt;h2 id="birds-eye-view-macro-profiling"&gt;Bird’s eye view: “macro” profiling&lt;/h2&gt;

&lt;p&gt;The first step in any kind of optimization is to assess the current situation. Profiling the test suite can help identify possible bottlenecks. Since we’re using RSpec, this first step was just running the whole suite with the &lt;code&gt;--profile=100&lt;/code&gt; flag, which outputs the 100 slowest examples and the 100 slowest example groups.&lt;/p&gt;

&lt;p&gt;Unfortunately, most of the 100 slowest tests were similarly slow, with the slowest completing at around 8 seconds and the fastest (of this “top 100 slowest” group) at around 4 seconds. In other words, they all took more or less the same time (at least in order of magnitude) to complete. There was no large “bottleneck” billboard to chase after, no single, minutes-long behemoth of a test to slay.&lt;/p&gt;

&lt;p&gt;The “example group” profiling section is where things got interesting. RSpec presents a list of groups (e.g. &lt;code&gt;describe&lt;/code&gt; blocks) alongside their total running time and average running time of each example in the group. Again the average running times were very much alike and formed a very mild and reasonable slope, with no individual group that could be singled-out as significantly slower than the rest. However, a few groups caught my attention because of their really long total running time, even though they had good or normal average running times:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-pseudo"&gt;Top 100 slowest example groups:
 Group 1
 3.02 seconds average (151.12 seconds / 50 examples) ./spec/…
 Group 2
 2.39 seconds average (143.25 seconds / 60 examples) ./spec/…
 Group 3
 1.7 seconds average (425.19 seconds / 250 examples) ./spec/…
 Group 4
 1.53 seconds average (145.27 seconds / 95 examples) ./spec/…
…
Group 38
 0.64877 seconds average (0.64877 seconds / 1 example) ./spec/…
 Group 39
 0.62933 seconds average (1.26 seconds / 2 examples) ./spec/…
 Group 40
 0.59483 seconds average (1.78 seconds / 3 examples) ./spec/…
 Group 41
 0.58132 seconds average (95.34 seconds / 164 examples) ./spec/…
 Group 42
 0.20273 seconds average (72.17 seconds / 356 examples) ./spec/…&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Groups 1 through 4 are clearly candidates for a closer look, but so are groups 41 and 42, which were buried deep in the report because of their relatively good average running times.&lt;/p&gt;

&lt;p&gt;Now that we had a list of candidates for examination, it was time to bring in the microscope.&lt;/p&gt;

&lt;h2 id="under-the-microscope-micro-profiling-withrubyprof"&gt;Under the microscope: “micro” profiling with RubyProf&lt;/h2&gt;

&lt;p&gt;The first spec we examined was Group 3, a large (250 examples) spec that took several minutes to run. After discarding the “obvious” possible culprits described in the literature review section above, we fired up &lt;a href="https://github.com/ruby-prof/ruby-prof"&gt;RubyProf&lt;/a&gt;, a MRI code profiler.&lt;/p&gt;

&lt;p&gt;It would be highly impractical to run a code profiler on a spec with 250 examples, so again we ran &lt;code&gt;rspec --profile&lt;/code&gt; on this individual file to get an overview of the examples in this spec. As could be expected, all the examples took a similar time to complete. So we chose a single example among the ones that took a bit more time to complete and dumped a RubyProf call tree at the end of the spec:&lt;/p&gt;

&lt;h3 id="group3specrb"&gt;group_3_spec.rb&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-ruby"&gt;&lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="s1"&gt;'does something'&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
  &lt;span class="o"&gt;...&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="no"&gt;RubyProf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;stop&lt;/span&gt;
  &lt;span class="n"&gt;printer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="no"&gt;RubyProf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="no"&gt;CallTreePrinter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;printer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="s2"&gt;"."&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;profile: &lt;/span&gt;&lt;span class="s2"&gt;"profile"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Examining the generated call tree with &lt;a href="http://kcachegrind.sourceforge.net/html/Home.html"&gt;KCacheGrind&lt;/a&gt;, we found out that IO waits (&lt;code&gt;IO#wait_readable&lt;/code&gt; in the screenshot) were responsible for most of the wall time for this particular spec:&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/images/goiabada/1*FjCiO6MvtgTo0pr533qZVw.jpg" alt="Call tree for a sampled spec example in group_3_spec.rb ."&gt;
Call tree for a sampled spec example in group_3_spec.rb .&lt;/p&gt;

&lt;p&gt;This heavy IO load was odd: all the HTTP requests were handled with VCR and there was no other blatant IO use such as file loading. Further examining VCR configuration lead to an interesting discovery.&lt;/p&gt;

&lt;h2 id="the-vcrcaveat"&gt;The VCR caveat&lt;/h2&gt;

&lt;p&gt;For those unfamiliar with &lt;a href="https://github.com/vcr/vcr"&gt;VCR&lt;/a&gt;, it is a ruby gem that records HTTP interactions so that automated tests don’t have to actual HTTP requests each run. The recorded interactions are persisted in disk and committed in the versioning system. In their own words:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Record your test suite’s HTTP interactions and replay them during future test runs for fast, deterministic, accurate tests.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are many ways to use VCR (insert_cassette, use_cassette, etc). The programmer that wrote this particular spec chose to hook VCR with the top-level describe block, as so:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;describe Foo, vcr: true do ...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Unlike insert_cassette and use_cassette, which require a cassette name argument, &lt;code&gt;vcr: true&lt;/code&gt; follows the spec’s tree structure, creating directories for example groups (describe/context blocks) and individual cassettes for each example.&lt;/p&gt;

&lt;p&gt;This might seem innocent enough, but Group 3 had 250 examples, most of which had some kind of HTTP interaction, resulting in over a hundred cassettes scattered over 40 or so folders. Many of the cassettes were identical, since there were examples that performed the same HTTP requests. For Group 3 alone, almost 2MB of cassettes were loaded! Despite surely being faster than making all the actual HTTP requests, loading this many files from disk was definitely an unnecessary IO strain — as the call tree showed.&lt;/p&gt;

&lt;p&gt;Fortunately, we can easily force VCR to record all interactions in the group within a single cassette by using the &lt;code&gt;:cassette_name&lt;/code&gt; option:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;describe Foo, vcr: { cassette_name: 'foo_spec' } do ...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The resulting new cassette for Group 3 had merely 33KB — orders of magnitude smaller than the previous cassettes. Reduced file size and the elimination of IO overhead caused by loading multiple scattered files significantly reduced spec speed: Group 3 now runs at around 110 seconds, nearly a quarter of the previous run time (425 seconds).&lt;/p&gt;

&lt;p&gt;While using a single cassette was not always possible, and not always relevant (in smaller specs, for example), there were a great many specs that suffered loading dozens or hundreds of cassettes.&lt;/p&gt;

&lt;h2 id="brute-forcing-factorygirl-helpers"&gt;Brute-forcing FactoryGirl helpers&lt;/h2&gt;

&lt;p&gt;We know that avoidable DB hits are one of the things that often slow down specs unnecessarily. In other words, tests commonly persist more data than they need to. One of the most easily traceable sources of persisted data are the FactoryGirl helpers (&lt;code&gt;create&lt;/code&gt; and also &lt;code&gt;build&lt;/code&gt;, which may persist data if associations are needed for that model).&lt;/p&gt;

&lt;p&gt;Understanding if a specific test really needs to persist something is hard. Reading the test and making sure it does not touch the DB is not enough because of indirect DB hits which may be buried arbitrarily deep within the call tree of any of the methods used in the test. When you have several thousand tests, it’s basically impossible to have that kind of knowledge over the entire suite. Unless you are reading the spec for a specific purpose (i.e. trying to improve your knowledge of a model or a behavior), spending so much effort towards understanding a spec also represents tremendous re-work — after all, “understanding a spec” is obviously a huge part of the work needed to build a spec.&lt;/p&gt;

&lt;p&gt;Thankfully, we don’t have to understand the entire spec suite: we just have to leverage all those man-hours put into writing them by assuming they are correct and making sure the tests pass. So we did a global find-and-replace, replacing&lt;code&gt;create&lt;/code&gt;with&lt;code&gt;build_stubbed&lt;/code&gt;, then we ran the whole suite and did a&lt;code&gt;git checkout&lt;/code&gt;on the failing specs. We repeated the process, now swapping &lt;code&gt;create&lt;/code&gt;with &lt;code&gt;build&lt;/code&gt;. After the process, our test suite was making several hundred DB hits less than before — for next to zero work.&lt;/p&gt;

&lt;h2 id="tidying-up"&gt;Tidying up&lt;/h2&gt;

&lt;p&gt;Beyond the specs themselves, there is an entire load process that also consumes time. There were at least half a dozen gems in the test group that were either unnecessary or no longer used. &lt;code&gt;spec_helper&lt;/code&gt; (and &lt;code&gt;rails_helper&lt;/code&gt;) are also loaded in each spec, and had several unused or unnecessary code. Even &lt;code&gt;pending&lt;/code&gt;tests can take a toll on the suite run time, since the entire testing apparatus (gems, spec_helper, etc) is still loaded for the test.&lt;/p&gt;

&lt;h2 id="final-results"&gt;Final results&lt;/h2&gt;

&lt;p&gt;With these three improvements, CI run time fell from 24 minutes to 14 minutes — a 41.6% improvement. The project repo is also 20MB smaller due to the merged cassettes. All in just a couple of days’ work!&lt;/p&gt;

&lt;p&gt;These were solid improvements and should be celebrated. However, 14 minutes is still far from ideal for any TDD-style development. At some point in the future, we might want to look into fine-tuning our CI configuration, using a slimmer&lt;code&gt;spec_helper&lt;/code&gt;(which is more or less untouched since we were in Rails 3, and loads all of Rails— we’re on Rails 5 now, so we should leverage &lt;code&gt;rails_helper&lt;/code&gt;if we can), and perhaps parallelizing some specs if possible.&lt;/p&gt;

&lt;p&gt;By &lt;a href="https://medium.com/@lbrito"&gt;Leonardo Brito&lt;/a&gt; on &lt;a href="https://medium.com/p/8f3ab05a33f9"&gt;September 4, 2017&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/@lbrito/improving-spec-speed-in-a-huge-old-rails-app-8f3ab05a33f9"&gt;Canonical link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Exported from &lt;a href="https://medium.com"&gt;Medium&lt;/a&gt; on May 1, 2019.&lt;/p&gt;
</content>
    <summary type="html">The story of how we improved our test suite speed by 41% in a huge, 6-year-old Rails project.</summary>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2017-06-19:/blog/2017/06/how-a-Unix-CLI-tool-made-me-care-about-software-feedback.html</id>
    <title type="html">How a Unix CLI tool made me care about software feedback</title>
    <published>2017-06-19T00:00:00Z</published>
    <updated>2017-06-19T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2017/06/how-a-Unix-CLI-tool-made-me-care-about-software-feedback.html" type="text/html"/>
    <content type="html">&lt;p&gt;Providing feedback is one of the most important parts of any software. Unfortunately, more often than not we tend to downplay or ignore the very simple yet crucial task of letting the user know what is going on. In this article I’ll use a short cautionary tale of how the lack of proper user feedback (and some laziness, I admit) almost cost me an entire HDD with years of personal data.&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/goiabada/1*RUsq0P9vGjQQvjpsbov09g.jpeg" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/goiabada/1*RUsq0P9vGjQQvjpsbov09g.jpeg" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/goiabada/1*RUsq0P9vGjQQvjpsbov09g.jpeg" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;Can you tell by the output of dd that the device will be completely and irrevocably wiped out? Hint: while the operation is running (i.e. before hitting CTRL+C), there _is no output.&lt;/div&gt;
  
&lt;/div&gt;

&lt;!-- more --&gt;

&lt;p&gt;It was late at night and I was trying to clean up an SD card with some vacation photos. However, a silly media-detection Ubuntu process had hanged at some point and locked down the card, impeding any write operations on it. So I did what any lazy developer would do at 1AM: hastily search StackOverflow and paste into the terminal the first possibly applicable code snippet. Albeit totally overkill, the answer was legit: zero-filling (writing &lt;code&gt;0&lt;/code&gt;s in the disk, effectively deleting data) the device with the &lt;code&gt;dd&lt;/code&gt; program. So I copy-pasted the command, hit Enter and waited — without properly understanding what I had just typed, I might add. The command seemed to take too long so I killed it after a few seconds. Much to my surprise, on reboot the OS wouldn’t load.&lt;/p&gt;

&lt;p&gt;The command I had copy-pasted from StackOverflow used &lt;code&gt;dev/sdb&lt;/code&gt; as the destination device to be zero-filled, which must have been the original poster’s SD card device name. Unfortunately for me, that was the name of my secondary HDD, while my SD card was probably something like &lt;code&gt;/dev/sdc&lt;/code&gt;. The &lt;code&gt;dd&lt;/code&gt; program had zero-filled the beginning of the HDD, which contains the partition table information, making it unmountable. After some tinkering around I removed the HDD entry from &lt;code&gt;/etc/fstab&lt;/code&gt; and was able to boot into Ubuntu. After some research I found out the correct tools to restore the disk’s partition table and everything was fine (except for the hours of sleep I lost on that day).&lt;/p&gt;

&lt;p&gt;The first and obvious lesson learned here is, of course, don’t copy-paste and immediately run things from the Internet. It is potentially dangerous and just plain bad practice. The thing is, though, users will &lt;em&gt;always&lt;/em&gt; do something that is bad practice. Real-world users will find themselves tired and frustrated at 1AM, like myself, and will do stuff they’re “not supposed to do”. That is why feedback is so important: a simple confirmation dialog saying something like “this command will completely overwrite the destination file at device x, are you sure you want to continue?” might have made me think twice, and I might have saved a couple of hours of sleep.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;But you should know better! It is so obvious that &lt;code&gt;sudo dd if=/dev/zero of=/dev/sdb bs=512&lt;/code&gt; will copy &lt;code&gt;0x00&lt;/code&gt;s to the output file, &lt;code&gt;/dev/sdb&lt;/code&gt;, which happens to be the address of your secondary HDD, and will thus erase all your data!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I totally should, and now that I am well-rested and have good hindsight on the subject it does feel like an embarrassingly naive mistake that I’d never want to write about on a blog post and share with my peers. But at the time I was unfamiliar with &lt;code&gt;dd&lt;/code&gt;, and also tired and impatient. Users will often be tired, impatient or frustrated, and won’t always know exactly what some command, button or link is intended to do on your software. Even extremely knowledgeable and competent people &lt;a href="https://twitter.com/gitlabstatus/status/826591961444384768"&gt;will sometimes make almost unbelievable mistakes&lt;/a&gt;: let’s just call it human nature.&lt;/p&gt;

&lt;p&gt;So the real lesson to be learned here isn’t “scold your user for being stupid”, but rather “make sure your software gives the user proper feedback”. We can’t change human nature, and people will always do something wrong for many different reasons. I am really not talking about anything complex, but just very basic, simple things: it costs nothing to add a confirmation dialog, a well-thought-of label or an informative modal to your software. As developers, we are constantly tempted to thinking that if the code works, then the feature is done; but the user might not even know that the feature is ready and working unless they have proper feedback.&lt;/p&gt;

&lt;p&gt;Giving the user some feedback around critical sections of your software will go a long way. At worst, good feedback is harmless, and at best it will save someone from deleting their HDD.&lt;/p&gt;

&lt;p&gt;By &lt;a href="https://medium.com/@lbrito"&gt;Leonardo Brito&lt;/a&gt; on &lt;a href="https://medium.com/p/656f5fe3f6b8"&gt;June 19, 2017&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/@lbrito/how-a-unix-cli-tool-made-me-care-about-software-feedback-656f5fe3f6b8"&gt;Canonical link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Exported from &lt;a href="https://medium.com"&gt;Medium&lt;/a&gt; on May 1, 2019.&lt;/p&gt;
</content>
    <summary type="html">Providing feedback is one of the most important parts of any software. This is a tale of how the lack of proper feedback (and some laziness) almost cost me an entire HDD filled with personal data.</summary>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2017-05-08:/blog/2017/05/when-postgres-is-not-enough.html</id>
    <title type="html">When Postgres is not enough</title>
    <published>2017-05-08T00:00:00Z</published>
    <updated>2017-05-08T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2017/05/when-postgres-is-not-enough.html" type="text/html"/>
    <content type="html">&lt;p&gt;What happens when your project’s RDBMS is just not enough to deal with unexpectedly huge amounts of data?&lt;/p&gt;

&lt;p&gt;You could try to de-normalize some tables here and there to avoid unnecessary JOINs, create a few indexes, implement some kind of pagination or even pre-process the data into a more palatable format. However, if you did all that and it still was not enough, the “natural impulse” is to give up on the RDBMS altogether and just use Elasticsearch. Sounds like a no-brainer, right?&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;Well, what if you can’t jump into Elasticsearch right away? This is a more subtle but much more realistic scenario which we faced in a project here at Guava.&lt;/p&gt;

&lt;p&gt;In an ideal world, the correct or most appropriate technology stack would be chosen at the very beginning of all projects. For many reasons that are all too familiar to any seasoned developer, this is very seldom the case of any big software. The beginning of the project is precisely when you &lt;em&gt;least&lt;/em&gt; &lt;em&gt;know&lt;/em&gt; about the software requirements, thus it is often the point when seemingly innocuous choices are made which will cause great pain in the future. Once a wrong or sub-optimal choice is made, it will only get worse over time, &lt;em&gt;even before you even realize it is wrong&lt;/em&gt;. While the team is still confident that the solution is salvageable and can be fixed through careful optimizations and esoteric incantations — like we described above — the project is drifting further and further from completion, and a full stop would in fact be more productive.&lt;/p&gt;

&lt;p&gt;Back to our concrete case: we had a massive and steadily-increasing set of highly normalized data on a RDB. Adding insult to injury, the input data was incredibly sparse, with batch-like data peaks followed by several hours with little activity. The data distribution made it clear that we would have to do some kind of pagination. The following figure shows our input data distribution, where the x-axis units represent 1 hour worth of data, and the y-axis represents the data count in that hour:&lt;/p&gt;

&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/goiabada/0*71RikbZX7I55dU74.jpg" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/goiabada/0*71RikbZX7I55dU74.jpg" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/goiabada/0*71RikbZX7I55dU74.jpg" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;As we can see in the chart, many 1-hour blocks had nearly no data at all, and the great majority of blocks had less than 100k entries. A few blocks have a huge amount of data, surpassing 700k entries. This chart has 1-hour precision, but if we zoom in and investigate the same data set with 1-minute precision, the chart would look more or less the same. This is only a small sample of the data set we were dealing with: the full set had over 20M entries, growing at a rate of about 1M entries per month.&lt;/p&gt;

&lt;p&gt;We were asked to do some fairly standard BI stuff: report and dashboard generation. Because of the widely normalized data and the sheer amount of rows, we were forced into pre-processing very early in project. Our pre-processing hugely reduced the amount of data that the web app would actually have to process and display to the final user, but introduced a new set of problems: we were effectively de-normalizing the entire data set, which had to be done continuously and in small, manageable chunks (so we could tackle the sparse data distribution), and we were now responsible for maintaining the integrity and completeness of the optimized data. The processing also added a new layer of complexity to the application.&lt;/p&gt;

&lt;p&gt;Our solution was viable, but only just; when it came to our attention that the upstream data was actually versioned and non-monotonic, we quickly realized that simply adjusting our solution to deal with versioning would add so much complexity and extra load to the RDB that would render it unusable. The moment had come when we would start feeling the itch to make the jump to Elasticsearch — at least for the optimized, pre-processed layer which we described above, sitting on top of the raw input data. In theory, Elasticsearch would neatly handle all of our major issues: massive data volume, versioning and high normalization.&lt;/p&gt;

&lt;p&gt;However, the ideal world and the real world are worlds apart. We had already spent a few months worth of effort in developing and validating an arc of SQL logic that by nature depends on the initial choice of using an RDBMS as data source. Other teams of developers were already working on the same branch. Time constraints made it impossible for us to re-write the whole relational ecosystem, but at the same time continuing to use Postgres on the new layer was definitively not an option due to the versioning and performance issues we were facing.&lt;/p&gt;

&lt;h2 id="the-right-compromise"&gt;The right compromise&lt;/h2&gt;

&lt;p&gt;We made a compromise and decided to use &lt;em&gt;both Postgres and Elasticsearch on the same layer of pre-processed data&lt;/em&gt;. This unusual solution allowed us to carry on using most of the valuable relational logic already done, while leveraging Elasticsearch’s speed and document versioning. Sitting beneath the dual-store layer we still had the same sturdy Postgres store with raw data.&lt;/p&gt;

&lt;p&gt;The solution can be summarized as follows: Elasticsearch was set up upstream of our project, and an index was created with denormalized data gathered from our most-used tables. On our side, during pre-processing, we queried the Elasticsearch index and dumped the documents in a temporary table that existed only during pre-processing. The rest of the process remained more or less unaltered, allowing us to quickly solve 2 of the 3 major issues we were facing: performance and version treatment. The price to pay was code complexity, the third issue, which increased significantly with this approach. Additionally, this approach nudged the project closer to the ideal solution (using only Elasticsearch in the performance-critical parts of report generation) — other compromises might have gone the other way, digging deeper into RDBMS esotericism and further from the ideal solution.&lt;/p&gt;

&lt;p&gt;Our solution was far from ideal: it was a compromise, but it was the most beneficial compromise that could be made at the moment. Sometimes a silver bullet like Elasticsearch seems to exist, but for many reasons it is just out of your reach for the time being and you must settle for a compromise. In these cases, make sure to make the right one.&lt;/p&gt;

&lt;p&gt;By &lt;a href="https://medium.com/@lbrito"&gt;Leonardo Brito&lt;/a&gt; on &lt;a href="https://medium.com/p/237b723be442"&gt;May 8, 2017&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/@lbrito/when-postgres-is-not-enough-237b723be442"&gt;Canonical link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Exported from &lt;a href="https://medium.com"&gt;Medium&lt;/a&gt; on May 1, 2019.&lt;/p&gt;
</content>
    <summary type="html">What happens when your project’s RDBMS is just not enough to deal with unexpectedly huge amounts of data? Well, what if you can’t jump into Elasticsearch right away?</summary>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2017-03-20:/blog/2017/03/dont-obsess-over-code-dry.html</id>
    <title type="html">Don't obsess over code DRYness</title>
    <published>2017-03-20T00:00:00Z</published>
    <updated>2017-03-20T00:00:00Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2017/03/dont-obsess-over-code-dry.html" type="text/html"/>
    <content type="html">&lt;p&gt;Being clever is a good thing for a developer. Ingenuity allows us to write software that solves complex real-world problems. However, “clever” &lt;em&gt;code&lt;/em&gt; is not always a good thing. In many cases — I dare say in &lt;em&gt;most&lt;/em&gt; cases — it is a very bad thing. I consciously try to avoid writing code that might be seen as “clever”. The smart thing to do is trying hard not to be smart (yes, very &lt;a href="http://literarydevices.net/war-is-peace/"&gt;1984&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Developers tend to see themselves (quite indulgently) as smart people. Not many people understand what we do, and society sees a developer as a kind of modern wizard, writing unreadable magic spells in a small metal box. In reality, though, we are not half as smart as we think: for instance, if you are a developer, you are certainly familiar with the frustration of trying to understand some cryptic piece of code that seemed perfectly reasonable and straightforward when you wrote it a couple of months earlier.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;It is a given that any programmer will have to deal with the frustration of trying to understand complex code countless times throughout their career. Of course, there are genuine reasons to write complex code: sometimes there are strict hardware limitations, such as in the early ages of electronic computers, and sometimes the problem’s domain itself is inherently complex. However, if you’re reading this, you’re probably not &lt;a href="https://en.wikipedia.org/wiki/ENIAC#Role_in_the_hydrogen_bomb"&gt;living in the 1940s and working on a hydrogen bomb&lt;/a&gt;, and it’s more likely you’re working on some kind of web app using a dynamic programming language and a helpful framework, so you can probably take advantage of that and keep things simple.&lt;/p&gt;

&lt;p&gt;Why, then, do we insist in writing unnecessarily complex and cryptic code when we don’t absolutely need to? As it turns out, there are many reasons (although very few of them are good): to impress your boss and coworkers, to feel smart or proud of yourself, to challenge yourself, or just out of boredom. Those are all very real reasons why people deliberately write complex code. But people also write complex code unintentionally, and while actually trying their best to do the opposite: this is what happens when a programmer misinterprets genuine programming guidelines and good practices.&lt;/p&gt;

&lt;p&gt;A good example of this is the &lt;a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself"&gt;Don’t Repeat Yourself (DRY)&lt;/a&gt; guideline, repeated as a mantra in some Computer Science classes and in the industry. As humans, we have the gift and tendency towards recognizing patterns — it is what allows us to recognize a familiar face, appreciate music and understand languages, just to name a few examples. We also recognize patterns in source code, which we refactor following the DRY principle.&lt;/p&gt;

&lt;p&gt;The thing about pattern recognition, though, is that humans are very good at it — sometimes &lt;em&gt;too good&lt;/em&gt;. This can easily lead to the overuse of an otherwise perfectly healthy programming guideline. Psychology has a term for pattern recognition overuse/misuse: &lt;a href="https://en.wikipedia.org/wiki/Apophenia"&gt;apophenia&lt;/a&gt;. It is what happens when you see a pattern that doesn’t really exist, like a gambler “identifying” patterns in lottery tickets or a programmer “identifying” patterns in source code which aren’t really there.&lt;/p&gt;

&lt;p&gt;The original definition of DRY, from Hunt and Thomas’ &lt;em&gt;The Pragmatic Programmer&lt;/em&gt;, states:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Every piece of knowledge must have a single, unambiguous, authoritative representation within a system.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A &lt;em&gt;piece&lt;/em&gt; of knowledge hints at a well-defined knowledge &lt;em&gt;unit&lt;/em&gt;, which may vary in size depending on the specifics of your code. When a programmer sees a pattern in sections of code that do not belong to a common pattern — that is, are not within the same &lt;em&gt;piece of knowledge&lt;/em&gt; -, and still decides to refactor those sections by extracting them into a common piece of code, then &lt;em&gt;different&lt;/em&gt; &lt;em&gt;pieces&lt;/em&gt; of knowledge are being mashed together, and thus are DRY is not being applied at all.&lt;/p&gt;

&lt;p&gt;Let’s use an example to illustrate a misuse of DRY. Suppose you’re working on a car dealership software. The dealership sells and services a single car model, offering 3 scheduled maintenances at 10, 30 and 50 thousand miles:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-ruby"&gt;&lt;span class="c1"&gt;# Example 1&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Car&lt;/span&gt;
  &lt;span class="kp"&gt;include&lt;/span&gt; &lt;span class="no"&gt;Checkups&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maintenance_10k&lt;/span&gt;
    &lt;span class="n"&gt;check_break_fluid&lt;/span&gt;
    &lt;span class="n"&gt;check_battery_terminals&lt;/span&gt;
    &lt;span class="n"&gt;check_engine_oil&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maintenance_30k&lt;/span&gt;
    &lt;span class="n"&gt;check_break_fluid&lt;/span&gt;
    &lt;span class="n"&gt;check_battery_terminals&lt;/span&gt;
    &lt;span class="n"&gt;check_engine_oil&lt;/span&gt;
    &lt;span class="n"&gt;check_spare_wheel&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maintenance_50k&lt;/span&gt;
    &lt;span class="n"&gt;check_break_fluid&lt;/span&gt;
    &lt;span class="n"&gt;check_battery_terminals&lt;/span&gt;
    &lt;span class="n"&gt;check_engine_oil&lt;/span&gt;
    &lt;span class="n"&gt;check_spare_wheel&lt;/span&gt;
    &lt;span class="n"&gt;check_gearbox&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At a first glance, you may be tempted to DRY the code by extracting the three methods which are called in all maintenances: &lt;code&gt;check_break_fluid&lt;/code&gt;, &lt;code&gt;check_battery_terminals&lt;/code&gt; and &lt;code&gt;check_engine_oil&lt;/code&gt;. The resulting code is more concise:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-ruby"&gt;&lt;span class="c1"&gt;# Example 2&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Car&lt;/span&gt;
  &lt;span class="kp"&gt;include&lt;/span&gt; &lt;span class="no"&gt;Checkups&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maintenance_10k&lt;/span&gt;
    &lt;span class="n"&gt;basic_maintenance&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maintenance_30k&lt;/span&gt;
    &lt;span class="n"&gt;basic_maintenance&lt;/span&gt;
    &lt;span class="n"&gt;check_spare_wheel&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maintenance_50k&lt;/span&gt;
    &lt;span class="n"&gt;basic_maintenance&lt;/span&gt;
    &lt;span class="n"&gt;check_spare_wheel&lt;/span&gt;
    &lt;span class="n"&gt;check_gearbox&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="kp"&gt;private&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;basic_maintenance&lt;/span&gt;
    &lt;span class="n"&gt;check_break_fluid&lt;/span&gt;
    &lt;span class="n"&gt;check_battery_terminals&lt;/span&gt;
    &lt;span class="n"&gt;check_engine_oil&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;DRYing produces this new basic_maintenance method. It is not very descriptive: while &lt;code&gt;maintenance_*&lt;/code&gt; methods convey exactly what it they are expected to do (i.e. “perform a 10, 30 or 50 thousand miles maintenance”), &lt;code&gt;basic_maintenance&lt;/code&gt; is kind of an arbitrary name we made up that could mean anything. It is an abstract creation that exists only for our convenience and does not represent anything in the real world.&lt;/p&gt;

&lt;p&gt;Let’s imagine a very simple change in the requirements: suppose we no longer need to check the break fluid on the 10 thousand miles checkup. Now we must decide between removing &lt;code&gt;check_break_fluid&lt;/code&gt;from &lt;code&gt;basic_maintenance&lt;/code&gt; and adding the check only to the 30k and 50k maintenances, thus reducing &lt;code&gt;basic_maintenance&lt;/code&gt;’s effectiveness at avoiding repetition, or eliminating the method altogether and going back to how things were in Example #1.&lt;/p&gt;

&lt;p&gt;Although Example #1 has more repetitions than Example #2, it is arguably more readable and descriptive. It is also less likely to break if there are changes in the requirements like we just described. Bear in mind that this is a very simple example: all the methods do is call other methods that don’t take any parameters; there is no argument passing, no state changes, no transformations, etc. A more complex example would increase even further the abstractness and complexity of DRYing the code.&lt;/p&gt;

&lt;p&gt;A little repetition is preferable to a code that was DRYed incorrectly or excessively. If the abstraction resulting from DRY refactoring is more painful to understand than the alternative (going through a few repeated code sections), then the programmer was probably suffering of apophenia, seeing code patterns that did not exist — and thus not applying DRY correctly. Sandi Metz &lt;a href="https://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction"&gt;summarizes this very clearly&lt;/a&gt; in her 2014 RailsConf talk:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“Prefer duplication over the wrong abstraction.”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;With that said, there is another acronym that complements DRY: &lt;a href="https://codeshelter.wordpress.com/2011/04/07/dry-and-damp-principles-when-developing-and-unit-testing/"&gt;DAMP&lt;/a&gt;. DAMP means &lt;em&gt;descriptive and meaningful phrases&lt;/em&gt;. Although directed mostly at tests, the general principle of acknowledging the value of descriptiveness applies to all sorts of code_:_ good code is not too repetitious, but is also not too abstract and generic. Sometimes there is no general case to be abstracted, there are just a couple of concrete, specific cases which you should treat as concrete, specific cases.&lt;/p&gt;

&lt;p&gt;The purpose of DRY, DAMP and all the other fancy programming principles is to guide us towards crafting better code. If the result of DRYing something is a code that is more complex and less maintainable, then we have defeated the purpose of DRY. Programming principles are not laws of nature that will guarantee better code, which means that they are not universally applicable. More than knowing how to cleverly refactor and DRY a code, it is important to know &lt;em&gt;when&lt;/em&gt; something should be DRYed and when it should be left alone.&lt;/p&gt;

&lt;p&gt;By &lt;a href="https://medium.com/@lbrito"&gt;Leonardo Brito&lt;/a&gt; on &lt;a href="https://medium.com/p/e9ecc5224ff"&gt;March 20, 2017&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/@lbrito/dont-obsess-over-code-dryness-e9ecc5224ff"&gt;Canonical link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Exported from &lt;a href="https://medium.com"&gt;Medium&lt;/a&gt; on May 1, 2019.&lt;/p&gt;
</content>
    <summary type="html">Being clever is a good thing for a developer. Ingenuity allows us to write software that solves complex real-world problems. However, “clever” _code_ is not always a good thing.</summary>
  </entry>
  <entry>
    <id>tag:lbrito1.github.io,2015-10-28:/blog/2015/10/building-a-shared-library-in-c-and-using-it-in-a-python-program.html</id>
    <title type="html">Building a shared library in C and using it in a Python program</title>
    <published>2015-10-28T22:01:29Z</published>
    <updated>2015-10-28T22:01:29Z</updated>
    <link rel="alternate" href="https://lbrito1.github.io/blog/2015/10/building-a-shared-library-in-c-and-using-it-in-a-python-program.html" type="text/html"/>
    <content type="html">&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/codedeposit/2015/10/pathfinding.png?w=660" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/codedeposit/2015/10/pathfinding.png?w=660" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/codedeposit/2015/10/pathfinding.png?w=660" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;pathfindin&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Figure 1&lt;/p&gt;

&lt;p&gt;How do old-time languages such as C, Fortran and others survive in a world with Python, Ruby and so on?&lt;/p&gt;

&lt;p&gt;There is plenty legacy code still around which need maintaining, of course. And there are (will always be?) a few specific applications where low level is needed. But one of the great things with software is building upon old stuff using new tools, which brings us to our topic today: building a shared library containing some of our C stuff and using it in nice and comfy Python. Figure 1 shows an example of what we can achieve by using graphical tools available in Python to improve our existing code’s text-based output. More on that later on.&lt;/p&gt;

&lt;p&gt;For our purposes, we consider shared libraries as a collection of compiled objects condensed into a single file, which may then be called by other software. This is, of course, a simplification. A longer discussion about shared and static libraries can be found in [1].&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;In this post, we will define a Python wrapper for the &lt;a href="https://codedeposit.wordpress.com/2014/02/14/3/"&gt;linked list&lt;/a&gt; data structure we coded a couple of years ago. This is an interesting use case, by the way: writing wrappers for a shared library coded in a lower-level programming language may have several advantages. You could decide to scrap the C code and implement everything from scratch in the higher-level language, but then you’re throwing away precious time spent implementing and testing in the lower-level language. The lower-level library may also offer significantly better performance than a native implementation in the higher-level language. Also, as we will see, writing a wrapper is actually exceedingly simple, as is building the shared library itself.&lt;/p&gt;

&lt;p&gt;The process of compiling all our algorithms and data structures into shared libraries was actually didactic, because it enforced some good practices. Our project’s structure is now much more organized and sane; makefiles were written; (many) bugs were found, memory leaks were unveiled and fixed. Overall, our code was improved.&lt;/p&gt;

&lt;p&gt;Here’s an example makefile that compiles one of our shared libraries, data_structures:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-make"&gt;&lt;span class="nv"&gt;CFLAGS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;-fPIC&lt;/span&gt; &lt;span class="nt"&gt;-DPYLIB&lt;/span&gt;
&lt;span class="nv"&gt;LDFLAGS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;-shared&lt;/span&gt; &lt;span class="nt"&gt;-Wl&lt;/span&gt;,-soname,data_structures
&lt;span class="nv"&gt;PYDEP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;-I&lt;/span&gt;/usr/include/python2.7 &lt;span class="nt"&gt;-lpython2&lt;/span&gt;.7
&lt;span class="nv"&gt;SRCS&lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="nf"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;wildcard&lt;/span&gt; src/&lt;span class="k"&gt;*&lt;/span&gt;.c&lt;span class="nf"&gt;)&lt;/span&gt;

&lt;span class="nl"&gt;../shared/data_structures.so&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
	gcc &lt;span class="nv"&gt;$(SRCS)&lt;/span&gt; &lt;span class="nv"&gt;$(LDFLAGS)&lt;/span&gt; &lt;span class="nv"&gt;$(PYDEP)&lt;/span&gt; &lt;span class="nv"&gt;$(CFLAGS)&lt;/span&gt; &lt;span class="nv"&gt;$&amp;lt;&lt;/span&gt; &lt;span class="nt"&gt;-o&lt;/span&gt; &lt;span class="nv"&gt;$@&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;-fPIC&lt;/strong&gt; stands for position independent code. In short, compiled code will use a global offset table to reference addresses, which allows multiple processes to share the same code. See [2] and [3] for some nice discussions and explanations about PIC and why it is needed in this context. &lt;strong&gt;-Wl&lt;/strong&gt; says that the next option should be passed as an argument to the linker. The option in this case is &lt;strong&gt;-soname,data_structures&lt;/strong&gt;, which defines the shared object’s name (hence &lt;em&gt;soname&lt;/em&gt;) as the string “data_structures”.&lt;/p&gt;

&lt;p&gt;Now let’s define the Python interface. Let’s start by &lt;strong&gt;init&lt;/strong&gt;.py, where we’ll load the shared libraries:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;ctypes&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;ct&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pdb&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;set_trace&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt;
&lt;span class="n"&gt;libutil&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ct&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CDLL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'shared/utils.so'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;libdata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ct&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CDLL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'shared/data_structures.so'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;libsort&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ct&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CDLL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'shared/sorting.so'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;__all__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'ct'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'bp'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'libutil'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'libdata'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'libsort'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href="https://docs.python.org/2/library/ctypes.html"&gt;ctypes&lt;/a&gt; is the native way of loading shared libraries in Python. As the official doc states: “ctypes is a foreign function library for Python. It provides C compatible data types, and allows calling functions in DLLs or shared libraries. It can be used to wrap these libraries in pure Python.”&lt;/p&gt;

&lt;p&gt;All functions present in the &lt;strong&gt;data_structures&lt;/strong&gt; library will be available in the Python object &lt;strong&gt;libdata&lt;/strong&gt;. Now here’s the linked list wrapper:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;cdepo&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;intref&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ct&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;byref&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ct&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c_int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;LinkedList&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
	&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;libdata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;libutil&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compare_integer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ct&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ct&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c_int&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

	&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="n"&gt;libdata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_ll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;intref&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

	&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add_many&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;libdata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search_ll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;intref&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

	&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;delete&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="n"&gt;libdata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;delete_ll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;intref&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

	&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__str__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="n"&gt;libdata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print_ll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
		&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;""&lt;/span&gt;

	&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="n"&gt;libdata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;delete_linked_list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And here’s a simple test that shows how we can use the Python class which uses our C functions underneath:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;cdepo.data_structures.linked_list&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;"Creating list"&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinkedList&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;"Adding 3 and 5"&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;"Adding 10, 20 and 30"&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_many&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;"Deleting 5"&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;delete&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;"Deleting 3 (list head)"&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;delete&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;"Deleting 30 (list tail)"&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;delete&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;"Deleting the remaining elements (list should be empty)"&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;delete&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;delete&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href="https://gist.github.com/lbrito1/a1d0a1e60c126792d598"&gt;Here’s&lt;/a&gt; the output.&lt;/p&gt;

&lt;p&gt;In our little example we only used the same functionalities we already had in C. However, one of the great advantages of accessing a library with another language is using tools that are specific to that language. As an example, let’s use Matplotlib to render some images that improve our &lt;a href="%7B%%20link%20_posts/2014-04-06-shortest-path-part-i-dijkstras-algorithm.markdown%20%%7D"&gt;pathfinding&lt;/a&gt; visualization. We built Python wrappers for the necessary functions (Graph-related and Dijkstra’s algorithm) in the same fashion as we did with Linked List. Here’s the resulting script:&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;cdepo.data_structures.graph&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;

&lt;span class="n"&gt;dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;

&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;matrix_graph&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;put_rect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;put_rect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;put_rect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;finish&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;dists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pathfind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;finish&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bgfx_mat&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;interpolation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'nearest'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'Oranges'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dist_mat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dists&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;interpolation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'nearest'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'gist_rainbow'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;:————————-:|:————————-:&lt;/p&gt;
&lt;div class="image-box stretch"&gt;
  &lt;div&gt;
    &lt;a href="/blog/assets/images/codedeposit/2015/10/figure_1.png?w=300" target="_blank"&gt;
      &lt;img class="lazy " data-src="/blog/assets/images/codedeposit/2015/10/figure_1.png?w=300" alt=""&gt;
      &lt;noscript&gt;
        &lt;img src="/blog/assets/images/codedeposit/2015/10/figure_1.png?w=300" alt=""&gt;
      &lt;/noscript&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    &lt;div class="image-caption"&gt;figure_&lt;/div&gt;
  
&lt;/div&gt;

&lt;p&gt;Figure 2&lt;/p&gt;

&lt;p&gt;Figure 2’s left picture shows the shortest path (using Dijkstra’s algorithm) between the two highlighted vertices. The brown rectangles represent “walls”, i.e. high-cost vertices. Right picture shows the distances to the starting node of each vertex. Obviously a great improvement over &lt;a href="%7B%%20link%20_posts/2014-03-18-burgergfx-simple-2d-graphics.markdown%20%%7D"&gt;simple text-based output&lt;/a&gt; which we use within C (the picture at the beginning of this post illustrates the difference).&lt;/p&gt;

&lt;p&gt;Bottom line, compiling your stuff into shared libraries is a great way of reusing code and breathing a whole new life into it.&lt;/p&gt;

&lt;p&gt;As always, all the code used in this post is on &lt;a href="https://github.com/lbrito1/cstuff"&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="bibliography"&gt;Bibliography&lt;/h2&gt;

&lt;p&gt;[1] Beazley, David M. et al., &lt;a href="http://cseweb.ucsd.edu/~gbournou/CSE131/the_inside_story_on_shared_libraries_and_dynamic_loading.pdf"&gt;The inside story on shared libraries and dynamic loading&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href="https://www.technovelty.org/c/position-independent-code-and-x86-64-libraries.html"&gt;Position Independent Code and x86-64 libraries &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] http://stackoverflow.com/questions/7216244/why-is-fpic-absolutely-necessary-on-64-and-not-on-32bit-platforms&lt;/p&gt;

&lt;p&gt;[4] &lt;a href="http://www.akkadia.org/drepper/dsohowto.pdf"&gt;http://www.akkadia.org/drepper/dsohowto.pdf&lt;/a&gt;&lt;/p&gt;
</content>
    <summary type="html">How do old-time languages such as C, Fortran and others survive in a world with Python, Ruby and so on?</summary>
  </entry>
</feed>

